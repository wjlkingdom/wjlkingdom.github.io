<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Blog &#8211; Yanran's Attic</title>
<meta name="description" content="Natural Language Processing, Machine Learning, Deep Learning, R">
<meta name="keywords" content="Natural Language Processing, Machine Learning, Deep Learning, Text Mining, R, Theano, GPU">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Blog">
<meta property="og:description" content="Natural Language Processing, Machine Learning, Deep Learning, R">
<meta property="og:url" content="http://yanran.li/index.html">
<meta property="og:site_name" content="Yanran's Attic">





<link rel="canonical" href="http://yanran.li/">
<link href="http://yanran.li/feed.xml" type="application/atom+xml" rel="alternate" title="Yanran's Attic Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://yanran.li/assets/css/main.min.css">
<!-- Webfonts -->
<link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://yanran.li/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://yanran.li/images/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://yanran.li/images/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://yanran.li/images/favicon.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://yanran.li/images/favicon.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://yanran.li/images/favicon.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://yanran.li/images/favicon.png">
<link rel="shortcut icon" href="/images/favicon.ico"/>
<link rel="bookmark" href="/images/favicon.ico"/>



<!-- MathJax -->
<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://yanran.li">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://yanran.li/images/avatar.jpg" alt="Yanran Li photo" class="author-photo">
					<h4>Yanran Li</h4>
					<p>Research Assistant at PolyU</p>
				</li>
				<li><a href="http://yanran.li/about/">Learn More</a></li>
				<li>
					<a href="mailto:yanranli.summer@gmail.com"><i class="icon-envelope"></i> Email</a>
				</li>
				
				
				<li>
					<a href="http://weibo.com/summerrlee"><i class="icon-weibo"></i> Weibo</a>
				</li>
				
				<li>
					<a href="https://www.linkedin.com/profile/view?id=233043238"><i class="icon-linkedin"></i> LinkedIn</a>
				</li>
				<li>
					<a href="http://github.com/niangaotuantuan"><i class="icon-github-alt"></i> GitHub</a>
				</li>
				
				
				
				
<!-- 				 -->
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://yanran.li/posts/">All Posts</a></li>
				<li><a href="http://yanran.li/categories/">All Categories</a></li>			
				<li><a href="http://yanran.li/tags/">All Tags</a></li>			
			</ul>
		</li>
		<li><a href="http://yanran.li/categories/#peppypapers">PaperNotes</a></li><li><a href="http://yanran.li/guestbook">GuestBook</a></li><li><a href="http://yanran.li/friends">Friends</a></li><li><a href="https://web.cs.dal.ca/~vlado/nlp/" class="external">NLP links</a></li><li><a href="http://www.reddit.com/r/machinelearning" class="external">r/machinelearning</a></li><li><a href="https://plus.google.com/communities/112866381580457264725" class="external">Deep Learning G+</a></li>
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  
  
    <div class="entry-image">
      <img src="http://yanran.li/images/bg_main.jpg" alt="Blog">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>Yanran's Attic</h1>
      <h2>Blog</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2015-04-18T00:00:00+08:00"><a href="http://yanran.li/peppypapers/2015/04/18/DeepWalk-Online-Learning-of-Social-Representations.html">April 18, 2015</a></time></span><span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://yanran.li/peppypapers/2015/04/18/DeepWalk-Online-Learning-of-Social-Representations.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://yanran.li/peppypapers/2015/04/18/DeepWalk-Online-Learning-of-Social-Representations.html" rel="bookmark" title="DeepWalk Online Learning of Social Representations" itemprop="url">DeepWalk Online Learning of Social Representations</a></h1>
    
  </header>
  <div class="entry-content">
    <p>《<strong>DeepWalk: Online Learning of Social Representations</strong>》是一篇我个人非常喜欢的论文，不仅提出了一个想法，更展示了这个想法的可行性和可能空间。提出的想法是利用网络结构信息将用户表示为低维实值向量，学出来的表示是最重要的，因为有了表示就可以用来加在许多其他任务上。</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2015-04-17T00:00:00+08:00"><a href="http://yanran.li/machinelearning/2015/04/17/collections-of-tips-for-machine-learning.html">April 17, 2015</a></time></span><span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://yanran.li/machinelearning/2015/04/17/collections-of-tips-for-machine-learning.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://yanran.li/machinelearning/2015/04/17/collections-of-tips-for-machine-learning.html" rel="bookmark" title="Collections of Tips for Machine Learning" itemprop="url">Collections of Tips for Machine Learning</a></h1>
    
  </header>
  <div class="entry-content">
    <p>收集了一些我觉得真的有用实战机器学习的文章。比如如何调参，比如会遇到什么真实数据带来的问题，如何 debug，如何 speed-up。长期更新。</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2015-04-15T00:00:00+08:00"><a href="http://yanran.li/life/2015/04/15/how-i-manage-my-knowledge.html">April 15, 2015</a></time></span><span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://yanran.li/life/2015/04/15/how-i-manage-my-knowledge.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://yanran.li/life/2015/04/15/how-i-manage-my-knowledge.html" rel="bookmark" title="我是这样进行知识管理的" itemprop="url">我是这样进行知识管理的</a></h1>
    
  </header>
  <div class="entry-content">
    <p>写这篇博客是前几天看到别人分享 pluskid 的<a href="http://freemind.pluskid.org/misc/knowledge-accumulate/">《关于知识整理、积累与记忆》</a>，加之我自己一直以来也很关注这件事，也有一点点心得。整理出来自己回顾，和大家讨论。</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2015-04-12T00:00:00+08:00"><a href="http://yanran.li/misc/2015/04/12/github-pages-categories-with-jekyll.html">April 12, 2015</a></time></span><span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://yanran.li/misc/2015/04/12/github-pages-categories-with-jekyll.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://yanran.li/misc/2015/04/12/github-pages-categories-with-jekyll.html" rel="bookmark" title="Github Pages Categories with Jekyll" itemprop="url">Github Pages Categories with Jekyll</a></h1>
    
  </header>
  <div class="entry-content">
    <p>When Jekyll is friendly to tags in post, it is not that .</p>

<p>There are three questions that you’ll encounter using categories: (1) Multiple word category name; (2) Multi categories; (3) Archive posts by a specific category.</p>

<h2 id="multiple-word-category-name">Multiple word category name</h2>

<p>When we want to name a category using multiple words (more than 1 word, contains spaces), jekyll will defaulty generate a permalink with the form http://domain.com/category/year/month/title and result in urls with dashes. It is the case and not bugs. We cannot easily change the permalink settings for the way Jekyll generates urls for posts in the config form but this can be done with a plugin.</p>

<p><strong>But, Github Pages just forbid any plugins</strong>. Thus, it will work on your own server other than blogs on Github Pages. Therefore, I suggest no spaces in category names.  </p>

<h2 id="multi-categories">Multi categories</h2>

<p>The second question is we sometimes want to category one post into multiple categories, like tags. Here we need to be careful with the frontmatters. Jekyll requires that Markdown files have front-matter defined at the top of every file. And for categories, it provides two formats of frontmatters. In the Jekyll’s <a href="http://jekyllrb.com/docs/frontmatter/#predefined-global-variables">documentations</a>, both <em>category</em> and <em>categories</em> are available.</p>

<p>Is there any differences? Sure. When we just need only one category, we can use both </p>

<p><code>
category: This is one category
</code></p>

<p>and </p>

<p><code>
categories: This is one category
</code></p>

<p>But, you’ve may got that when comes to multiple categories, we can <strong>only</strong> use <em>categories</em> and carefully using two formats below:
<code>
categories
  - This is one category
  - This is another category
</code></p>

<p>or the square brackets way:
<code>
categories: ['The is a category', 'This is another category']
</code></p>

<p>It makes sense.</p>

<p>So, the further question is, are <em>category</em> and <em>categories</em> really same when only one category? Actually, they don’t have the same effect on post object. When declaring <em>category</em>, post.category (string) and post.categories (array) are set. When declaring <strong>categories</strong>, only post.categories is set. Be careful!</p>

<h2 id="archive-posts-by-a-specific-category">Archive posts by a specific category</h2>

<p>But problems still show up when we want to archive the posts by a specific category. We may first try something like this:</p>

<p><code> 
{% for post in site.categories.'This is one category' %}  
...  
{% endfor %}     
</code></p>

<p>or this:</p>

<p><code>
{% for post in site.posts | where: 'category','This is one category' %}    
...    
{% endfor %}       
</code></p>

<p>If you tried, you failed. It is because you cannot put a filter on a loop. You have to capture first, then loop:</p>

<p><code>
{% capture myposts %} { { site.posts where: 'category','This is one category' } }       
{% endcapture %}         
{% for post in myposts %}     
...        
{% endfor %}       
</code>    </p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2015-04-11T00:00:00+08:00"><a href="http://yanran.li/peppypapers/2015/04/11/speed-of-mini-batch-sgd.html">April 11, 2015</a></time></span><span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://yanran.li/peppypapers/2015/04/11/speed-of-mini-batch-sgd.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://yanran.li/peppypapers/2015/04/11/speed-of-mini-batch-sgd.html" rel="bookmark" title="Speed of Mini-Batch SGD" itemprop="url">Speed of Mini-Batch SGD</a></h1>
    
  </header>
  <div class="entry-content">
    <p>This post comes from a friend’s question, that he says sometimes mini-batch SGD converges more slowly than single SGD. </p>

<p>Let’s begin with what these two kinds of method are and where they differ. Here notice that mini-batch methods come from batch methods.</p>

<h2 id="batch-gradient-descent">Batch gradient descent</h2>

<p><img src="/images/sgd_batch.png" alt="figures from L´eon Bottou" /></p>

<p><strong>Batch gradient descent</strong> computes the gradient using the whole dataset, while Stochastic gradient descent (SGD) computes the gradient using a single sample. This is great for convex, or relatively smooth error manifolds. In this case, we move <em>directly</em> towards an optimum solution, either local or global. </p>

<h3 id="pros">pros</h3>

<ol>
  <li>Great for convex, or relatively smooth error manifolds because it <em>directly</em> towards to the optimum solution.</li>
</ol>

<h3 id="cons">cons</h3>

<ol>
  <li>Using the whole dataset means that it is updating the parameters using all the data. Each iteration of the batch gradient descent involves a computation of the average of the gradients of the loss function over the entire training data set. So the computation cost matters.</li>
</ol>

<h2 id="stochastic-gradient-descent">Stochastic gradient descent</h2>

<p>While Batch gradient descent computes the gradient using the whole dataset, <strong>Stochastic gradient descent (SGD)</strong> computes the gradient using a single sample. </p>

<h3 id="pros-1">pros</h3>

<ol>
  <li>
    <p>Obviously SGD’s computationally a whole lot faster. </p>
  </li>
  <li>
    <p>Single SGD works well <strong>better than</strong> batch gradient descent <em>when the error manifolds that have lots of local maxima/minima</em>.</p>
  </li>
</ol>

<h3 id="cons-1">cons</h3>

<ol>
  <li>Sometimes, with the computational advantage, it should perform many more iterations of SGD, making many more steps than conventional batch gradient descent. </li>
</ol>

<h2 id="mini-batch-sgd">mini-batch SGD</h2>

<p><img src="/images/sgd_minibatch.png" alt="figures from L´eon Bottou" /></p>

<p>There comes the compromise of this two kinds of methods. When the batch size is 1, it is called stochastic gradient descent (GD).
When you set the batch size to 10 or to some extend larger, this method is called <strong>mini-batch SGD</strong>. Mini-batch performs better than true stochastic gradient descent because when the gradient computed at each step uses more training examples, mini-batches tend to average a little of the noise out that single samples inherently bring. Thus, the amount of noise is reduced when using mini-batches. Therefore, we usually see smoother convergence out of local minima into a more optimal region. </p>

<p>Thus, the batch size matters for the balance. We primally want the size to be small enough to avoid some of the poor local minima, and large enough that it doesn’t avoid the global minima or better-performing local minima. Also, a pratical consideratio raises from tractability that each sample or batch of samples must be loaded in a RAM-friendly size.</p>

<p>So let’s be more clear:</p>

<h2 id="why-should-we-use-mini-batch">Why should we use mini-batch?</h2>

<ol>
  <li>It is small enough to let us implement vectorization in RAM.</li>
  <li>Vectorization brings efficiency.</li>
</ol>

<h2 id="disadvantage-of-mini-batch-sgd">Disadvantage of mini-batch SGD</h2>
<p>is the difficulty in balancing the batch size <script type="math/tex">b</script>. </p>

<p>However, in the paper <a href="http://link.springer.com/article/10.1007%2Fs10107-012-0572-5"><em>Sample size selection in optimization methods for machine learning</em></a>, the author points out that though large mini-batches are preferable to reduce the
communication cost, they may slow down convergence rate in practice. And Mu Li in this <a href="http://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf"><em>papar</em></a> is dealing with this problem.</p>

<h2 id="ref">Ref</h2>

<p>[1]Bottou, Léon. <em>Large-scale machine learning with stochastic gradient descent.</em> Proceedings of COMPSTAT’2010. Physica-Verlag HD, 2010. 177-186.</p>

<p>[2]Bottou, Léon. <em>Online learning and stochastic approximations.</em> On-line learning in neural networks 17.9 (1998): 142.</p>

<p>[3]Li, Mu, et al. <em>Efficient mini-batch training for stochastic optimization.</em> Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014.</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-12-31T00:00:00+08:00"><a href="http://yanran.li/misc/2014/12/31/2014%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93.html">December 31, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://yanran.li/misc/2014/12/31/2014%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://yanran.li/misc/2014/12/31/2014%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93.html" rel="bookmark" title="2014年度总结" itemprop="url">2014年度总结</a></h1>
    
  </header>
  <div class="entry-content">
    <p>例行总结，主要回顾下半年。</p>

<h2 id="section">技术</h2>

<ol>
  <li>这半年工作中最大的体会就是维护自己常用的脚本、模板库、工作流程是多么值得的事情啊！直观地可参阅《geeks and repetitive tasks》那张图。</li>
  <li>做爬虫的时候被 beautifulsoup4 的一些奇怪行为坑的啊，暂时用 lxml 代替（scrapy）。</li>
  <li>被说 Python 写的很 Java，我觉得好像用起 pyquery 会好很多呢！</li>
  <li>关于爬虫这件事，发现了一些“爬虫的艺术”的文章(http://blog.urx.com/urx-blog/2014/9/4/the-science-of-crawl-part-1-deduplication-of-web-content)蛮有趣的。比如 bloom filter 众所周知的去重错判，我暂时考虑用 dict 方式做精确去重，当然了缺陷就是内存膨胀。</li>
  <li>在老板的莫名信任下，搭建了实验室的分布式环境。坑没有想象中多，稍微折腾了一下 native library 的问题。</li>
</ol>

<h2 id="section-1">学术</h2>

<p>年底三个月来到新的实验室，老板最初是希望我能多 support 师兄师姐。于是自己变得很 multi-task，经常周一周二优化师兄的代码，周三周四来帮学姐做些 experiment。确实充实，而且大大提高了自己的动手能力。在这种 task-driven 下，自己的效率提高了不少，也不能说因为占用了自己的思考时间而全无收获呢。</p>

<p>年底整理 Evernote/bookmark/待读论文 Tutorial 有感：1、sequential 的回顾学习，温故知新是一定的；2、完全掌握了的东西，把它们 pop out 自己的待读列表里，成就感棒棒的；3、除了硬知识的收获，更多地可以关注自己软思维的成长。</p>

<ol>
  <li>被 AAAI 拒了一次，虽然老板说很大原因是赶上的 reviewer 都不是做我这个方向的。但我觉得还是自己的 paper 有很多可以被人 argue 的部分吧，不够完善，不然，总是可以瑕不掩瑜的。</li>
  <li>本来对于现在在做的事情不是很着急了，因为刚来的时候自己是实验室里进度最慢的，完全连 model 都没定，想法也不成熟。结果两个月下来开组会，老板忽然说，觉得我是最有希望 2月份 发 ACL 的。我 faint 啊……不过我还是会努力的！</li>
  <li>现在对于学术这件事也很迷茫了，有很多时候真的无法说服自己相信自己在做一件很有价值的事情（但是手下这篇 paper 我确实觉得很 promising，当然，只是 idea promising，能不能做出来就……），而且开会的时候也总会听到老板说，这个会审稿什么风格，那个会又如何如何，感觉讨巧的东西太多。但大概这些想法只是因为我还太年轻太任性吧……</li>
  <li>写了一本科普性的书的 Chapter，特别特别惭愧，包括之前的一本书，辜负了很多人的期待和信任吧，自己一直拖稿。这次终于狠下心好好鞭笞了自己一把，最后的自我满意度能达到 80%。不过最近回忆起这本书的内容，我还是觉得自己眼界很小，写知识，就只能写出知识，却写不出一些宏大的 Vision，全局观。我觉得一是受限于自己本身的眼界，二是受限于日益退化的表达能力……想起熊辉大大微博说的话，</li>
</ol>

<blockquote>
  <p>@熊辉Rutgers
最近思考人或者企业的竞争，都是逐渐更复杂，更高级。有低到高： 1）拼勤奋； 2）拼智商 + 勤奋；3）拼 情商 + 智商 + 勤奋； 4） 拼 VISION （对未来趋势的把握） + 情商 + 智商 + 勤奋。最高等的一定有VISION</p>
</blockquote>

<h2 id="section-2">心智</h2>

<ol>
  <li>独自生活以后，确实感觉自由了很多。更深刻理解了一句话，自律等于自由。但是同时反过来想，这种自由的感觉并不是平白无故产生的。我的自由来自于我不怕失去的东西很少，和我想得到的东西都已经得到并且能轻易获得。任何选择都有得失，我们的今天，不仅是过去所有选择的总和，也是所有没得选择的总和，the way we miss our life is life。</li>
  <li>投资自己，不再成为 job task-driven 的奴隶。持续学习，每天自己 1-2 小时时间自由学习。坚持运动，关注健康，尤其是自己的心情:P</li>
  <li>从现在老板的身上看到了并且想学习的重要品质是与人争论的技巧。现在的我越来越喜欢能和我争论，最好能说服我的人。偏听则暗，至少这样我可以知道都有哪些相反的意见。再说回更简单的，就是要保持谦卑和好奇心。</li>
  <li>坚持了一些不美好生活里的英雄梦想，并且要继续坚持。</li>
  <li>自己的戾气和悲观少了很多，不安感和无力感逐渐消散。感觉自己越来越能这个世界和平相处，但庆幸自己还没有满足于他的表面的和平。我学会了发现越来越多的小确幸并且让它们变成了沉甸甸的砝码压在枕边和心头，但也同时拥有了目标实现时的巨大的成就感。我学会了享乐，也还没有忘记深刻的思考。</li>
</ol>

<h2 id="section-3">读书</h2>
<p>终于开始觉得，读书是一件很私人的事情。以后不再多做读书总结了。</p>

<h2 id="section-4">交易</h2>
<p>下半年交易中分了两个阶段，一个是因为从八月开始我就不断觉得美股要见顶了（这个预测现在反思是有点早了）我便逐渐减仓了美股，第二个就是自己时间上不再允许时差看盘，转而进入A股。
后来十月中旬的时候基本手头美股仓位基本只有 APPL 和 BITA 了。APPL 我从 83 一直拿到 118，我觉得这是2014年下半年自己做的最好的一只。A股这边自己就只是小追了一下军工和环保。对A股我还是比较谨慎，还是慢慢摸索。</p>

<ol>
  <li>每个人适合的交易方法不一样。不要总观察别人的操作，也不要自己频繁操作，而是去想清楚什么是重要的。</li>
  <li>我现在觉得风控是第一位的，A股之前的资金效应带动的版块，很多人都被套了，这种毫无逻辑支撑的大涨，看不清楚，宁愿不追。</li>
  <li>风控，资金。</li>
  <li>技术流中，找突破还是最稳健的办法。$MS 的图形值得收藏。</li>
</ol>

<p>============以下是上半年的交易心得总结=============</p>

<ol>
  <li>个股和指数是不同的，个股也分很多种，不能一概而论。有些个股适合长情，看好他们，有些个股适合一夜情，不可以多拿。</li>
  <li>能源是最棒我赚钱的股票，我从3月乌克兰事件一直拿到现在。长得稳，大盘好的时候它们涨的慢一些，大盘不好的时候涨幅更大。</li>
  <li>做熟悉的股票。这半年我交易次数最多的股票应该是 TNA，它的每个历史点关键位我都很熟悉，可以很快认清走势，做出判断，做了很长一段时间的 DT。每周都有一两天的时间可以让我赚 1-2 块。</li>
  <li>现在在测试一个自己的交易系统，这个系统不用主观判断，只用指标来决定买卖。买入在最容易止损的位置。年底再来总结。</li>
  <li>我验证了一下自己对于每次判断的正确率，大概在65%，还比较低，但是这不妨碍我交易。如果自己错了，就立刻止损，重新判断，所以我更看重买入的位置，要很容易止损。如果止损错了，又很容易买回。</li>
</ol>

<h2 id="section-5">运动</h2>

<ol>
  <li>坚持跑步而已。最喜欢的自虐方式了。</li>
  <li>后来找到了朋友陪我打壁球，虽然打得不咋样，不过很适合我这种暴力女啊。</li>
  <li>2015准备养成一些无器械运动的习惯。</li>
</ol>

<h2 id="section-6">习惯</h2>

<p>养成了一些习惯：
1. 每周五整理桌面
2. 每天两次查看 to-do
3. Wunderlist App 作为包括买菜清单、回家见人备忘等的 checklist，非常好用！
4. 周末洗床单被套、半周洗枕套</p>

<h2 id="section-7">人</h2>

<ol>
  <li>下半年最大的悲哀是被“骗”了钱，人生第一次吧。太过于信任对方。不过，金钱上的损失并不可怕，我也很快调整了心态，虽然我没吸取到啥教训的样子，但是并不会因此而不再相信人。希望自己永远有一个好心态面对不顺心，坚持自己的信心、梦想和好品质。</li>
  <li>生活禁不起推敲，不精确，不如意，不停止，力所难及。我带着悲哀，穿过荒唐，遇见时间，和你。</li>
  <li>“无论你怎么与他人控制距离，你依然会失去控制，因为这个世界上总有人能让你乖乖交心和伤心。from 韩寒《告白与告别》”</li>
  <li>前几天摘了尼采的话：“总之，问题全在于生命力： 你健康，你就热爱生命，向往人生的欢乐；你羸弱，你就念念不忘死亡，就悲观厌世。一个要在人世间有所建树的人最忌悲观主义：”看破红尘–这是巨大的疲劳和一切创造者的末日。”也就是那句，Pain is inevitable. Suffering is optional.</li>
  <li>一段看似荒唐却真实的感情，便看淡了很多事情。如果人可以有无限的体验，全知全能，那么人与人之间的真正理解是绝对可达的。如果上帝真是那全知全能的神，他爱世人，我便信。不过经此一遭，我越发豁达和潇洒了。荒唐有时，开心到老。</li>
</ol>

<h2 id="section-8">2015目标</h2>

<ol>
  <li>提高书面表达能力（写博客）</li>
  <li>无器械健身</li>
  <li>更有型一点</li>
  <li>学会自拍！</li>
  <li>每个月总结一次自己的技术收获，项目总结，而不要半年一次</li>
  <li>买一双 Christian Louboutin 红底鞋！</li>
</ol>

<p>最后还是谢谢这一年关心我、鼓励我、给过我机会、在我困惑时给我当头棒喝和温柔怀抱的路人和朋友们，因为你们和好书好吃的，才让我觉得生活越来越美好！</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-07-07T00:00:00+08:00"><a href="http://yanran.li/machinelearning/2014/07/07/ADMM-%E6%A1%86%E6%9E%B6%E5%B0%8F%E7%BB%93.html">July 07, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://yanran.li/machinelearning/2014/07/07/ADMM-%E6%A1%86%E6%9E%B6%E5%B0%8F%E7%BB%93.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://yanran.li/machinelearning/2014/07/07/ADMM-%E6%A1%86%E6%9E%B6%E5%B0%8F%E7%BB%93.html" rel="bookmark" title="ADMM 框架小结" itemprop="url">ADMM 框架小结</a></h1>
    
  </header>
  <div class="entry-content">
    <p>ADMM 是这两年很火的一个算法框架。</p>

<p>其主要的提出想法是为了“分布式”的解决一类问题，即：
<script type="math/tex">min f(x) + g(y)
s.t. Ax + By = c</script>
是等式约束（不可以是不等式）的最小化加和问题；且此时 f(x) 和 g(y) 是两类性质非常不同的函数，比如 LASSO 中的 1-norm 惩罚和前面的代价函数。</p>

<p>对于这个问题的 augmentated Lagrangian:</p>

<script type="math/tex; mode=display">L_{\rho}(x,z,y)=f(x)+g(z)+y^T(Ax+Bz-c)+(\rho/2)\|Ax+Bz-c\|_2^2</script>

<p>和对应的 update rule:</p>

<script type="math/tex; mode=display">x^{k+1}=\arg\min_x L_{\rho}(x,z^k,y^k)</script>

<script type="math/tex; mode=display">z^{k+1}=\arg\min_z L_{\rho}(x^{k+1},z,y^k)</script>

<script type="math/tex; mode=display">y^{k+1}=y^k+\rho(Ax^{k+1}+Bz^{k+1}-c)</script>

<p>可以看出，上图中的第一个式子和第二个式子是乘子法的一个特殊化；且第一个式子对应的是乘子法中的”x-minimization step”，第二个式子是所谓的“dual update”。</p>

<p>不同的是，乘子法是同时计算并最小化x,z：
<script type="math/tex">(x^{k+1},z^{k+1})=\arg\min_{x,z} L_{\rho}(x,z,y^k)</script></p>

<p>而ADMM中，x,z 是“alternated”计算的，它 decouples x-min step from y-min step。</p>

<p>这里可以看到，augmentated Lagrangian 虽然弱化了乘子法的强假设性，但 x-min step 引入了二次项而导致无法把 x 分开进行求解。所以 ADMM 也是就是期望结合乘子法的弱条件的收敛性以及对偶上升法的可分解求解性。</p>

<p>其实 ADMM 也是一个需要 trick 的框架：</p>

<ol>
  <li>
    <p>因为它的缺点是需要非常多步的迭代才能得到相对精确的解，这就好像是一阶算法。  </p>
  </li>
  <li>
    <p>另外，对 $\rho$ 的选择也很重要，非常影响收敛性；$\rho$ 太大，对于 min (f1+f2) 就不够重视；反之，则对于 feasibility 又不够重视。Boyd et al. (2010) 倒是给了实践中改变 $\rho$ 的策略，可是也没有证明它的收敛性。   </p>
  </li>
</ol>

<p>Boyd 在其<a href="http://web.stanford.edu/~boyd/papers/admm/">网站</a>上给出了一些例子。总结这里的几个例子，构造 ADMM 的形式，主要思想就是往直前的受约束的凸优化问题靠拢。
（1）对于只有传统损失函数没有正则项的（比如LAD, Huber Fitting），构造出一个 z。
（2）对于有约束的，比如 l1-norm 的约束，则把约束变成 g(x)，原始损失函数为 f(x)。若 f(x) 本身没有（比如Basic Pursuit），就构造成带有定义域约束的 f(x)（某种投影）；如果有，则就是比较一般化的损失+正则问题，这时候就基本是一个<script type="math/tex">f(x)+\lambda\|z\|_1</script>的形式。是非常自然的 ADMM。</p>

<p>所以，像广义线性模型和广义可加模型+正则等等都非常适合 ADMM。</p>

<p><em>Ref:</em></p>

<p>[1] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers, 2010.</p>

<p>[2] S. Boyd. Alternating Direction Method of Multipliers (Slides)</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-11-11T00:00:00+08:00"><a href="http://yanran.li/naturallanguageprocessing/2013/11/11/Relation-Extraction-with-Matrix-Factorization.html">November 11, 2013</a></time></span><span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://yanran.li/naturallanguageprocessing/2013/11/11/Relation-Extraction-with-Matrix-Factorization.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://yanran.li/naturallanguageprocessing/2013/11/11/Relation-Extraction-with-Matrix-Factorization.html" rel="bookmark" title="Relation Extraction with Matrix Factorization" itemprop="url">Relation Extraction with Matrix Factorization</a></h1>
    
  </header>
  <div class="entry-content">
    <p>This post is about the NAACL’13 Accepted Paper, <strong>Relation Extraction with Matrix Factorization and Universal Schemas</strong>. The talk is available on <a href="http://techtalks.tv/talks/relation-extraction-with-matrix-factorization-and-universal-schemas/58435/">techtalks</a>.</p>

<p>And then present some basic knowledge of <strong>Matrix Factorization</strong>. </p>

<h2 id="abstract">Abstract</h2>

<p>The paper studies techniques for inferring a model of entities and relations capable of performing basic types of semantic inference (e.g., predicting if a specific relation holds for a given pair of entities). The models exploit different types of embeddings of entities and relations.  </p>

<p>This problem is usually tackled either via distant weak supervision from a knowledge base (providing structure and relational schemas) or in a totally unsupervised fashion (without any pre-defined schemas). The present approach aims at combining both trends with the introduction of universal schemas that can blend pre-defined ones from knowledge bases and uncertain ones extracted from free text.  This paper is very ambitious and interesting. </p>

<h2 id="related-work">Related Work</h2>

<h3 id="relation-extraction">relation extraction</h3>

<p>There has been a lot of previous research on learning entailment (aka inference) rules (e.g., Chkolvsky and Pantel 2004; Berant et al, ACL 2011; Nakashole et al, ACL 2012). 
Also, there has been some of the very related work on embedding relations, e.g., Bordes et al (AAAI 2011), or, very closely related, Jenatton et al (NIPS 2012).</p>

<h3 id="matrix-factorization">Matrix Factorization</h3>

<p><strong>Matrix factorization</strong> as a technique of <em>Collaborative filtering</em> has been the
preferred choice for recommendation systems ever since Netflix million competition was held a few years back. Further, with the advent of news personalization, advanced search and user analytics, the concept has gained
prominence.</p>

<p>In this paper, columns correspond to relations, and rows correspond to entity tuples. By contrast, in (Murphy et al., 2012) columns are words, and rows
are contextual features such as “words in a local window.” Consequently, this paper’s objective is to complete the matrix, whereas their objective is to learn better latent embeddings of words (which by themselves again cannot capture any sense of asymmetry).</p>

<h2 id="save-storage">Save Storage</h2>

<p>Although the paper doesn’t explicit point out how common is it that a tuple shares many relations, it remains concern. The experiments seem to show that mixing data sources is beneficial. </p>

<h2 id="trends">Trends</h2>

<p>The researchers are ambitious to bridge knowledges bases and text for information extraction, and this paper seems to go along this trend.
However, the paper’s scheme is limited before complex named entity disambiguation is solved, since it relies on the fact that entities constituting tuples from the Freebase and tuples extracted from the text have been exactly matched beforehand.</p>

<h2 id="generalized-matrix-factorization">Generalized Matrix Factorization</h2>

<p>It has been a general machine learning problem formulated as:</p>

<h3 id="training-data">Training data</h3>
<ul>
  <li><strong>V</strong>: m x n input matrix (e.g., rating matrix)</li>
  <li>Z: training set of indexes in <strong>V</strong> (e.g., subset of known ratings)</li>
</ul>

<h3 id="parameter-space">Parameter space</h3>
<ul>
  <li><strong>W</strong>: row factors (e.g., m x r latent customer factors)</li>
  <li><strong>H</strong>: column factors (e.g., r x n latent movie factors)</li>
</ul>

<h3 id="model">Model</h3>
<ul>
  <li><script type="math/tex"> L_{ij}(W_{i*},H_{*j}) </script>: loss at element (<em>i</em>,<em>j</em>)</li>
  <li>Includes prediction error, regularization, auxiliary information, . . .</li>
  <li>Constraints (e.g., non-negativity)</li>
</ul>

<h3 id="find-best-model">Find best model</h3>

<script type="math/tex; mode=display"> \arg\min_{W,H}\sum_{(i,j)\in Z}L_{i,j}(W_{i*},H_{*j}) </script>

<h2 id="stochastic-gradient-descent-for-matrix-factorization">Stochastic Gradient Descent for Matrix Factorization</h2>

<p>Among the various algorithmic techniques available, the following are more
popular: <strong>Alternating Least Squares (ALS)</strong>， <strong>Non-Negative Matrix Factorization</strong> and <strong>Stochastic Gradient Descent (SGD)</strong>. Here I only presents SGD for MF.</p>

<p><strong>SDG</strong> is a well know technique which tends to compute direction of steepest descent and then takes a step in that direction. Among the variants include:</p>

<p>(a)Partitioned SGD: distribute without using stratification and run independently and in parallel on partitions (b)Pipelined SGD: based on ‘delayed update’ scheme (c)Decentralized SGD: computation in decentralized and distributed fashion</p>

<p>The main solution is as follows:</p>

<ul>
  <li>
    <p>Set <script type="math/tex"> \theta = (W,H) </script> and use</p>

    <p><script type="math/tex"> L(\theta)=\sum_{(i,j)\in Z}L_{ij}(W_{i*},H_{*j}) </script>,  <br />
  <script type="math/tex"> {L}'(\theta)=\sum_{(i,j)\in Z}{L}'_{ij}(W_{i*},H_{*j}) </script>,  <br />
  <script type="math/tex"> {\hat{L}}'(\theta,z)=N{L}'_{i_{z}j_{z}}(W_{i_{z}*},H_{*j_{z}}) </script>, where <script type="math/tex">N=\vert Z\vert</script></p>
  </li>
  <li>
    <p>SGD epoch</p>
    <ul>
      <li>Pick a random entry <script type="math/tex"> z \in Z </script></li>
      <li>Compute approximate gradient <script type="math/tex"> {\hat{L}}'(\theta,z) </script></li>
      <li>Update parameters <script type="math/tex"> \theta_{n+1}=\theta_{n}-\epsilon_{n}{\hat{L}}'(\theta,z) </script></li>
      <li>Repeat <script type="math/tex">N</script> times</li>
    </ul>
  </li>
</ul>

<h2 id="svm-vs-fm">SVM V.S. FM</h2>

<p><strong>FM</strong> is short for <a href="http://www.libfm.org/"><strong>Factorization Machine</strong></a>. Indeed, it can be interpreted as <strong>Factorization</strong> Methods and Support Vector <strong>Machine</strong>. It is firstly published by Steffen Rendle. </p>

<p>Factorization machines (FM) are a generic approach that allows to mimic most factorization models by feature engineering. This way, factorization machines combine the generality of feature engineering with the superiority of factorization models in estimating interactions between categorical variables of large domain. libFM is a software implementation for factorization machines that features stochastic gradient descent (SGD) and alternating least squares (ALS) optimization as well as Bayesian inference using Markov Chain Monte Carlo (MCMC).</p>

<p><img src="http://i.imgur.com/Kc7q9Pl.png" alt="" /></p>

<p>in SVM mode, <script type="math/tex"> y(x)=w\cdot x+b=w_{u}+w_{i}+...+b=\sum w_{i}x_{i}+b </script>, but original SVM fails with 2 main problems using here: <em>Real Value V.S. Classification</em>, and <em>Sparsity</em>.</p>

<p>in Factorization Machine mode, it is solved as: <script type="math/tex"> y(x)=\sum w_{i}x_{i}+\sum\sum(v_{i}\cdot v_{j})x_{i}x_{j} +b </script>. The second part in the formula is <strong>Factorization</strong>, where the transformation from original SVM to FM lies.</p>

<p><img src="http://i.imgur.com/bgOUxWh.png" alt="" />
<img src="http://i.imgur.com/eHhxEsb.png" alt="" /></p>

<h2 id="fm-vs-mf">FM V.S. MF</h2>

<ul>
  <li>FM: <script type="math/tex"> y(x)=\sum w_{i}x_{i}+\sum\sum(v_{i}\cdot v_{j})x_{i}x_{j} +b </script></li>
  <li>MF: <script type="math/tex"> y(x)=w_{u}+w_{i}+v_{u}\cdot v_{i} + b </script></li>
</ul>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-10-05T00:00:00+08:00"><a href="http://yanran.li/peppypapers/2013/10/05/incorporating-domain-knowledge-into-LDA.html">October 05, 2013</a></time></span><span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://yanran.li/peppypapers/2013/10/05/incorporating-domain-knowledge-into-LDA.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://yanran.li/peppypapers/2013/10/05/incorporating-domain-knowledge-into-LDA.html" rel="bookmark" title="Incorpating Domain Knowledge into LDA" itemprop="url">Incorpating Domain Knowledge into LDA</a></h1>
    
  </header>
  <div class="entry-content">
    <p>Recent years, there has been emerging research on knowledge-based models and methods. Researchers have tried in various ways to express/embed/structure the knowledge and then incorporating them into some existing models, among which is LDA (Latent Dirichlet Allocation). </p>

<p>For further detailed about LDA, please investigate through [Blei el al., 2003].  The basic idea and foundation of LDA is handling <strong>word co-occurrence</strong> pattern to discover the latent semantic meaning. The simple model has limited resolution to deeper latent sementics and thus the variations of LDA are bursting. One focus to expand LDA is how to incorporating more <strong>prior knowledge</strong> into it.</p>

<h2 id="types-of-prior-knowledge">Types of Prior Knowledge</h2>

<p>Basically, all types of knowledge incorporation is to change the prior distribution of Dirichlet setting in LDA. </p>

<p>a.	在传统 LDA 里，有两组先验，一种是文档~主题的先验，来自于一个对称的<script type="math/tex"> \mathcal Dir(\alpha) </script>；一种是主题~词汇的先验，来自于一个对称的<script type="math/tex"> \mathcal Dir(\beta) </script> ——都是 symmetric Dirichlet Distribution。所以按理，可以把这两种先验分别改成不对称的——这样就加入了更多的 knowledge 信息。</p>

<p>b.	在 Rethinking LDA 里一文<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>中，结合两种先验与两种不同的先验设定方法，可以得到以下四种组合：</p>

<ul>
  <li>AA：文档~主题分布和主题~词汇分布都采用非对称；</li>
  <li>AS：文档~主题分布采用非对称的先验，而主题~词汇分布采用对称的先验；</li>
  <li>SA：文档~主题分布采用对称的先验，而主题~词汇采用非对称；</li>
  <li>SS：文档~主题分布和主题~词汇分布都采用对称的先验。</li>
</ul>

<p>他们的实验发现其中 AS 的方法可以更高地提高 LDA 对于文本建模的能力。</p>

<p>c.  典型的打破“对称”文档~主题分布先验（AS）的几个model，有很好理解的 Twitter-LDA，也有 Behavior-LDA。同时，supervised-LDA 也可以看做一个非结构化的打破先验的方式，变形后有 SeededLDA（在两个层次的先验都通过设计加入了不对称信息）。</p>

<p>d.	除了通过<strong>直接</strong>地改变概率分布来加入先验的方法，这几年来开始有越来越多的研究者想将结构化的先验知识加入 LDA 。这种结构化的先验，不再是简单的 prior distribution，更可以倾向于称为“knowledge”。这样的研究之所以盛行，一方面是长期以来的结构化知识库已有很多（且因为还要继续建立知识图谱等，结构化仍将是未来的趋势），另一方面形式语言（逻辑语言）的表示的研究一直都没有停止。这种结构化的引入 knowledge 的方法，本质也是通过打破先验设定的 symmetric Dirichlet Distribution。下文将重点总结这方面的工作。</p>

<h2 id="domain-dependent-model">Domain-dependent Model：</h2>
<ul>
  <li>
    <p>CIKM’13 里，Zhiyuan Chen（也在 Bing Liu那里）的一篇 Discovering Coherent Topics<sup id="fnref:3"><a href="#fn:3" class="footnote">2</a></sup> 里将 incorporating knowledge 的研究分成了 domain-dependent 的和 domain-independent：前者是 expert 知道（普通人不一定熟悉，需要 expert 来参与编辑）的知识而且有知识领域限制，后者是各领域通用的一些知识。</p>
  </li>
  <li>
    <p>同样是上述文章，提到了<sup id="fnref:2"><a href="#fn:2" class="footnote">3</a></sup>,<sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup>,<sup id="fnref:5"><a href="#fn:5" class="footnote">5</a></sup>,<sup id="fnref:6"><a href="#fn:6" class="footnote">6</a></sup>,<sup id="fnref:7"><a href="#fn:7" class="footnote">7</a></sup> 的论文都是 domain-dependent knowledge-based 的。</p>
  </li>
  <li>
    <p>其中，Dirichlet Forest<sup id="fnref:2:1"><a href="#fn:2" class="footnote">3</a></sup> 和 Jerry Zhu 的 First-Order Logic<sup id="fnref:4:1"><a href="#fn:4" class="footnote">4</a></sup> 的形式化加入 domain-knowledge 的方法还是比较有代表性。前者是将领域内一定会一起出现（两个词的出现概率都很大或者都很小）的词和一定不能一起出现的词分别表示为 Must-Link 和 Cannot-Link，然后表示成树中的结点和结点之间的连接关系。但这个 Link 关系是可传递的，所以会导致“错误”的先验知识加入（CIKM’13 中提到了这点）。         </p>
  </li>
</ul>

<h2 id="domain-independent-model">Domain-independent Model：</h2>

<ul>
  <li>
    <p>按照 Zhiyuan Chen 的说法，他们在 CIKM’13 里提出的 GK-LDA<sup id="fnref:3:1"><a href="#fn:3" class="footnote">2</a></sup> 应该是第一个 domain-independent model。所以这个部分只谈他们的那篇论文（GK-LDA 是 General Knowledge 的缩写，即 domain-independent 的 knowledge）。</p>
  </li>
  <li>
    <p>在这篇论文里，他们的假设是，<em>there is a vast amount of available in online dictionaries or other resources that can be exploited in a model to generate more coherent topic</em>. 而通过 extract，就可以把这样的 lexical knowledge 提取成 a general knowledge base.</p>
  </li>
  <li>
    <p>他们采取的知识表达结构是 lexical relationships on words. 简称 <strong>LR-sets</strong>。LR-sets 有很多种关系，比如同义词、反义词，这篇文章中重点讲的是 adjective-attribute 这种 relationship，e.g. (expensive-price).</p>
  </li>
  <li>
    <p>他们提出的 GK-LDA 依然是 一种 LDA 的变形，而且是基于他们组再之前的工作——IJCAI’13 的 Leveraging Multi-Domain Prior Knowledge<sup id="fnref:8"><a href="#fn:8" class="footnote">8</a></sup> 里的 MDK-LDA。</p>
  </li>
</ul>

<h2 id="mdk-lda-b--mdk-lda--gk-lda">从 MDK-LDA (b) 到 MDK-LDA 到 GK-LDA：</h2>

<ul>
  <li>
    <p>主要总结 Zhiyuan Chen 的两篇工作，之前提过的 MDK-LDA 和 GK-LDA。</p>
  </li>
  <li>
    <p>MDK-LDA 是 multi-domain knowledge 的缩写，从思想上来看是一种 Transfer Learning 的想法，prior from other domain can help topic model in new domain.</p>
  </li>
  <li>
    <p>所谓的 multi-domain 可以通过 <strong>s-set</strong> 表示，比如 “light” has 2 s-set {light, heavy, weight} 和 {light, bright, luminanee}，表示出了 light 的两个词义。那么这个工作就是去 leverage 这个 s-sets。</p>
  </li>
  <li>
    <p>他们在 IJCAI’13 的那篇里<sup id="fnref:8:1"><a href="#fn:8" class="footnote">8</a></sup> 主要分析了 之前的 domain-knowledge 会遇到的两个大问题，MDK-LDA 解决了其中一个 adverse effect 的问题，而 GK-LDA 两个都解决了（还有一个是错误先验知识带来的问题）。MDK-LDA 解决的主要在 LDA 问题里，如何使得一些少见的词但是确实是同一个 set 里的词的低频不会影响 topic modeling 的学习（不仅仅用 TF-IDF 消除影响），那么他们认为 <em>the words in an s-set share a similar semantic meaning in the model should redistribute the probability masses over words in the s-set to ensure that they have similar probability under the same topic</em>. 这个思想使得他们在 MDK-LDA(basic) 之上加入了 GPU <sup id="fnref:9"><a href="#fn:9" class="footnote">9</a></sup>：像抽出小球再放回同颜色的球的思想一样，去改变同一个 s-set 里的 word 的dist.</p>
  </li>
  <li>
    <p>在 MDK-LDA 之上，解决第二个问题的就是 GK-LDA，也就是在 CIKM’13 里的那篇<sup id="fnref:3:2"><a href="#fn:3" class="footnote">2</a></sup>。MDK-LDA 没法避免当我们的先验 s-set 是错误的（这也是其他许多 domain-dependent model 的问题，必须保证我们的先验知识都是正确的）对 performance 的影响。 GK-LDA 加入了一个 word correlation matrix 的计算 和 加入一个 threshold，减少了 wrong LR-set 的的影响。</p>
  </li>
  <li>
    <p>其中加入 GPU 的思想，和 CRP 中如何改变人坐在具体某个餐桌的概率的思想是一致的（只是一个模型的不同解释）。</p>
  </li>
</ul>

<h2 id="footnotes">Footnotes</h2>

<ul>
  <li>
    <p>Transfer Learning 和 Active Learning、Online Learning 等等都有关系。这部分内容还没有系统学习过，之前一篇<a href="http://yanran.li/2013/07/covariate-shift-correction/">文章</a>也有提到这里的一个小坑。</p>
  </li>
  <li>
    <p>GPU<sup id="fnref:9:1"><a href="#fn:9" class="footnote">9</a></sup>，是 Generalized Polya Urn 的简称。搞懂 LDA 必须先学习的模型。将这个过程generalized, 可以推向Polya Urn’s Process。Polya Urn’s Model 是比较直观的理解 Dirichlet Process 的一种解释模型。模型中抽出球再放回就是对当前的多项分布进行抽样（同时不改变该分布），又放回一个同样的球就是依当前多项分布产生新的多项分布。假设从<script type="math/tex"> \mathcal Dir(\alpha, K) </script>中抽样，那么新产生的多项分布共有 K 个，其概率质量与当前多项分布成比例。K 个新产生的多项分布的加权平均与原多项分布是同分布的。而在之前的 CIKM’13 论文<sup id="fnref:3:3"><a href="#fn:3" class="footnote">2</a></sup>中就是通过改变每次放回的“球”（LR-set 里同一个 set 的词）的“颜色”和数量来改变 prior knowledge 的。这种思想感觉还是很赞的。</p>
  </li>
</ul>

<h2 id="reference">Reference</h2>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Hanna Wallach, David Mimno and Andrew McCallum. Rethinking LDA: Why Priors Matter. NIPS, 2009, Vancouver, BC. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun Hsu, Malu Castellanos, and Riddhiman Ghosh. Discovering Coherent Topics using General Knowledge. Proceedings of the ACM Conference of Information and Knowledge Management (CIKM’13). October 27 - November1, Burlingame, CA, USA. <a href="#fnref:3" class="reversefootnote">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote">&#8617;<sup>2</sup></a> <a href="#fnref:3:2" class="reversefootnote">&#8617;<sup>3</sup></a> <a href="#fnref:3:3" class="reversefootnote">&#8617;<sup>4</sup></a></p>
    </li>
    <li id="fn:2">
      <p>Andrzejewski, D., Zhu, X. and Craven, M. 2009. Incorporating domain knowledge into topic modeling via Dirichlet Forest priors. ICML, 25–32. <a href="#fnref:2" class="reversefootnote">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:4">
      <p>Andrzejewski, D., Zhu, X., Craven, M. and Recht, B. 2011. A framework for incorporating general domain knowledge into latent Dirichlet allocation using first-order logic. IJCAI, 1171–1177. <a href="#fnref:4" class="reversefootnote">&#8617;</a> <a href="#fnref:4:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:5">
      <p>Burns, N., Bi, Y., Wang, H. and Anderson, T. 2012. Extended Twofold-LDA Model for Two Aspects in One Sentence. Advances in Computational Intelligence. Springer Berlin Heidelberg. 265–275. <a href="#fnref:5" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Jagarlamudi, J., III, H.D. and Udupa, R. 2012. Incorporating Lexical Priors into Topic Models. EACL, 204–213 <a href="#fnref:6" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>Mukherjee, A. and Liu, B. 2012. Aspect Extraction through SemiSupervised Modeling. ACL, 339–348. <a href="#fnref:7" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:8">
      <p>Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun Hsu, Malu Castellanos, and Riddhiman Ghosh. Leveraging Multi-Domain Prior Knowledge in Topic Models. Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI’13). August 3-9, 2013, Beijing, China. <a href="#fnref:8" class="reversefootnote">&#8617;</a> <a href="#fnref:8:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:9">
      <p>David Mimno, Hanna Wallach, Edmund Talley, Miriam Leenders, Andrew McCallum. Optimizing Semantic Coherence in Topic Models. EMNLP (2011). <a href="#fnref:9" class="reversefootnote">&#8617;</a> <a href="#fnref:9:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-10-04T00:00:00+08:00"><a href="http://yanran.li/r/2013/10/04/loading-big-data-in-R.html">October 04, 2013</a></time></span><span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://yanran.li/r/2013/10/04/loading-big-data-in-R.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://yanran.li/r/2013/10/04/loading-big-data-in-R.html" rel="bookmark" title="Loading Big Data in R" itemprop="url">Loading Big Data in R</a></h1>
    
  </header>
  <div class="entry-content">
    <p>Although parallel techniques in R has been prevailing, I will only focus on Loading the complete data into RAM in R, that is to say, no Hadoop or similar. What other more I <em>won’t</em> mention in this post is about <a href="http://www.r-bloggers.com/big-data-analysis-for-free-in-r-or-how-i-learned-to-load-manipulate-and-save-data-using-the-ff-package/">manipulating and saving big data in R</a>, and parallel computing.</p>

<p>Just start with different implementations:</p>

<ul>
  <li>
    <p>load <strong>csv</strong> file and using <strong>ff</strong> package (Rtools)</p>

    <pre><code>  bigdata &lt;- read.csv.ffdf(file = ”bigdata.csv”, first.rows=5000, colClasses = NA)
</code></pre>

    <p><em>Notice</em> that ff package should be in Rtools on Windows.</p>
  </li>
  <li>
    <p>using <strong>sqldf()</strong> from <strong>SQLite</strong> </p>

    <p>this is a method from <a href="http://stackoverflow.com/a/1820610/1849063">StackOverflow</a>: using sqldf() to import the data into SQLite as a staging area, and then sucking it from SQLite into R</p>

    <pre><code>  library(sqldf)
  f &lt;- file("bigdf.csv")
  system.time(bigdf &lt;- sqldf("select * from f", dbname = tempfile(), file.format = list(header = T, row.names = F)))
</code></pre>
  </li>
  <li>
    <p>magic <strong>data.table</strong> and <strong>fread</strong></p>

    <p>it includes data.frame, but some of the syntax is different. Luckily, the <a href="http://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.pdf">documentation</a> (and the FAQ) are excellent.</p>

    <p>Read csv-files with the fread function instead of read.csv (read.table). It is faster in reading a file in table format and gives you feedback on progress.</p>

    <p><em>Notice</em> that fread() cannot directly read gzipped files and it comes with a big warning sign “not for production use yet”. One trick it uses is to read the first, middle, and last 5 rows to determine column types.</p>
  </li>
  <li>
    <p>optimized <strong>read.table()* with **colClasses</strong></p>

    <p>This option takes a vector whose length is equal to the number of columns in year table. Specifying this option instead of using the default can make ‘read.table’ run MUCH faster, often twice as fast. In order to use this option, you have to know the of each column in your data frame. - See more at <a href="http://simplystatistics.tumblr.com/post/11142408176/r-workshop-reading-in-large-data-frames#sthash.IpNe4GfP.dpuf">hear</a>.</p>

    <pre><code>  read.table("test.csv",header=TRUE,sep=",",quote="",  
                    stringsAsFactors=FALSE,comment.char="",nrows=n,                   
                    colClasses=c("integer","integer","numeric",                        
                                 "character","numeric","integer"))
</code></pre>
  </li>
  <li>
    <p>load a <strong>portion</strong> using <strong>nrows</strong></p>

    <p>Also you can read in only a portion of your file, to get a feel of the dataset.</p>

    <pre><code>  data_first_100 &lt;- read.table("file", header=T, sep="\t", stringsAsFactors=F, nrows=100)
</code></pre>
  </li>
  <li>
    <p>in summary</p>

    <p><a href="http://stackoverflow.com/a/15058684/1849063">Here</a> is a great comparison summary for the method above with their system time. I just copy the summary table below:</p>

    <pre><code>  ##    user  system elapsed  Method
  ##   24.71    0.15   25.42  read.csv (first time)
  ##   17.85    0.07   17.98  read.csv (second time)
  ##   10.20    0.03   10.32  Optimized read.table
  ##    3.12    0.01    3.22  fread
  ##   12.49    0.09   12.69  sqldf
  ##   10.21    0.47   10.73  sqldf on SO
  ##   10.85    0.10   10.99  ffdf
</code></pre>
  </li>
</ul>

<p>See more in <a href="http://www.theodi.org/blog/fig-data-11-tips-how-handle-big-data-r-and-1-bad-pun">11 Tips on How to Handle Big Data in </a>.</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->


<div class="pagination">
  
    Previous
  
  <ul class="inline-list">
    <li>
      
        <span class="current-page">1</span>
      
    </li>
    
      <li>
        
          <a href="http://yanran.li/page2">2</a>
        
      </li>
    
  </ul>
  
    <a href="http://yanran.li/page2" class="btn">Next</a>
  
</div><!-- /.pagination -->
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2015 Yanran Li. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://yanran.li/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://yanran.li/assets/js/scripts.min.js"></script>

          

</body>
</html>