<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <title></title>
    <description>Natural Language Processing, Machine learning, Deep learning</description>    
    <link></link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Datasets</title>
                
        
          <description>&lt;p&gt;Here are some selected datasets commonly used in Machine Learning related tasks.&lt;/p&gt;

</description>
        
        <pubDate>Fri, 05 Jun 2015 00:00:00 +0800</pubDate>
        <link>/resource/2015/06/05/datasets.html</link>
        <guid isPermaLink="true">/resource/2015/06/05/datasets.html</guid>
      </item>
    
      <item>
        <title>Adapations and Variations of Word2vec</title>
                
        
          <description>&lt;p&gt;word2vec 作为一个已经被广为流传的工具，其优点已不必多说。那么它有什么缺陷和不足呢？其实其作者 Mikolov 是一个非常典型的工程型选手，实用主义，什么简单方便有效就用什么；导致 word2vec 作为一个简单的模型，其忽略了很多文本中的其他信息。那么这些其他信息都有什么呢？&lt;/p&gt;

</description>
        
        <pubDate>Sat, 23 May 2015 00:00:00 +0800</pubDate>
        <link>/papers/2015/05/23/Adapations-and-Variations-of-Word2Vec.html</link>
        <guid isPermaLink="true">/papers/2015/05/23/Adapations-and-Variations-of-Word2Vec.html</guid>
      </item>
    
      <item>
        <title>Improving Word Representations via Global Context and Multiple Word Prototypes</title>
                
        
          <description>&lt;p&gt;《Improving Word Representations via Global Context and Multiple Word Prototypes》这篇论文意在用全文信息辅助局部信息和多个词向量共同表示一个词的方法，增强语义。不止如此，在我看来，这篇论文最重要的地方有四个：首先的思想是 word disambiguation 在 context level；第二个是用 C&amp;amp;W 的 ranking loss 会比以前的 log-likelihood 训练速度快。第三个是把 local 和 global 的两种 score 设计成 NN 中的两个 part，分别用一层 hidden layer 学习。但是这里他们只用了简单的加法，而没有线性权重参数 $\alpha$。后人许多改进了 $\alpha$，还做了些参数对比展示实验结果。不过本质没区别。第四个是他们并没有直接用 SGD，二是用了 1000 的 mini-batch L-BFGS，这点好像追随的人不多。&lt;/p&gt;

</description>
        
        <pubDate>Sun, 03 May 2015 00:00:00 +0800</pubDate>
        <link>/papers/2015/05/03/Improving-Word-Representations-via-Global-Context-and-Multiple-Word-Prototypes.html</link>
        <guid isPermaLink="true">/papers/2015/05/03/Improving-Word-Representations-via-Global-Context-and-Multiple-Word-Prototypes.html</guid>
      </item>
    
      <item>
        <title>DeepWalk Online Learning of Social Representations</title>
                
        
          <description>&lt;p&gt;《&lt;strong&gt;DeepWalk: Online Learning of Social Representations&lt;/strong&gt;》是一篇我个人非常喜欢的论文，不仅提出了一个想法，更展示了这个想法的可行性和可能空间。提出的想法是利用网络结构信息将用户表示为低维实值向量，学出来的表示是最重要的，因为有了表示就可以用来加在许多其他任务上。&lt;/p&gt;

</description>
        
        <pubDate>Sat, 18 Apr 2015 00:00:00 +0800</pubDate>
        <link>/papers/2015/04/18/DeepWalk-Online-Learning-of-Social-Representations.html</link>
        <guid isPermaLink="true">/papers/2015/04/18/DeepWalk-Online-Learning-of-Social-Representations.html</guid>
      </item>
    
      <item>
        <title>Collections of Tips for Machine Learning</title>
                
        
          <description>&lt;p&gt;收集了一些我觉得真的有用实战机器学习的文章。比如如何调参，比如会遇到什么真实数据带来的问题，如何 debug，如何 speed-up。长期更新。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://jmetzen.github.io/2015-01-29/ml_advice.html&quot;&gt;&lt;strong&gt;Advice for applying Machine Learning&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;主要集中在如何观察数据来选择方法。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://vitalflux.com/machine-learning-debug-learning-algorithm-regression-model/&quot;&gt;&lt;strong&gt;How to Debug Learning Algorithm for Regression Model&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;主要都是讲回归中遇到的各种“预期不符”的结果。配合 ESL 第二章和第三章内容看效果加成。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.weibo.com/p/1001603816330729006673&quot;&gt;&lt;strong&gt;训练深度神经网络的时候需要注意的一些小技巧&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这篇是综合翻译，没给出都从哪节选的。我收集的英文版在下面：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://deeplearning4j.org/trainingtricks.html&quot;&gt;&lt;strong&gt;Training Tricks from Deeplearning4j&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;deeplearning4j 的 googlegroups 也很推荐。这篇其实干货不多，但是也有一些了。包括对于训练的理解，并不全是干货般的总结。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.weibo.com/p/1001603799166017998138&quot;&gt;&lt;strong&gt;Suggestions for DL from Llya Sutskeve&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hinton 亲传弟子介绍深度学习的实际 tricks，包括data, preprocessing, minibatches, gradient normalization, learning rate, weight initialization, data augmentation, dropout和ensemble。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://fb56552f-a-62cb3a1a-s-sites.googlegroups.com/site/deeplearningworkshopnips2014/71.pdf?attachauth=ANoY7cp_eDwTXPm6iWHdBRhlIsgPASEAwkW-exLSOsz467mge7zLCkBMWznOu_G90vGVtqNvXOusc4z6cC6hEnHk6YzHtuEr_kyU0fyme7asaECN0zvoNwDk5258CueoB6fY3WtLvbJzYok1xiIeWSFYtk5mKXCXFDMI6djwhjCX1xi0GEEv_x7uMQwTdQlDItZ3kgLnZ2RjctQmIXDCu58fS3Wby4vWX3CkhMIf_EpCXx7jDn_M2SM%3D&amp;amp;attredirects=0&quot;&gt;&lt;strong&gt;Efficient Training Strategies for Deep Neural Network Language Models&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;讨论了如何设置 batch-size, initial learning rate, network initialization，但最有趣的结论应该是：普通的 deep feed-forward architecture比recurrent NN 在 model long distance dependency 效果和效率都更好。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5333-large-scale-l-bfgs-using-mapreduce.pdf&quot;&gt;&lt;strong&gt;Large-scale L-BFGS using MapReduce&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;NIPS’14 的论文，简单并行化 LBFGS里面的双循环（最耗时，计算量巨大）。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everyting-side-by-side/&quot;&gt;&lt;strong&gt;特征工程选择系列&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;特征工程系列文章：Part1.单变量选取 Part2.线性模型和正则化 Part3.随机森林 Part4.稳定性选择法、递归特征排除法(RFE)及综合比较。有 Python 代码。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.weibo.com/p/1001603795687165852957&quot;&gt;&lt;strong&gt;机器学习代码心得之​有监督学习的模块&lt;/strong&gt;&lt;/a&gt;
&lt;a href=&quot;http://www.weibo.com/p/1001603795714256832384&quot;&gt;&lt;strong&gt;机器学习代码心得之迭代器和流水处理&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;新一代大神陈天奇怪的系列文章，有兴趣的直接顺着看吧。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/&quot;&gt;&lt;strong&gt;STOCHASTIC GRADIENT BOOSTING: CHOOSING THE BEST NUMBER OF ITERATIONS&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Kaggle达人YANIR SEROUSSI告诉你如何选择Stochastic Gradient Boosting的训练最佳iteration超参数。不过我比较存疑，因为如果条件允许，当然迭代的越多越好……&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.eeshyang.com/papers/KDD14Jubjub.pdf&quot;&gt;&lt;strong&gt;Large-Scale High-Precision Topic Modeling on Twitter&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Twitter 高级研究员的 KDD’14论文。有不少实用技巧，比如短文本特征，LR结果概率化修正，正样本抽样，PU学习后负样本选取。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf&quot;&gt;&lt;strong&gt;How transferable are features in deep neural
networks?&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;也是争议比较大的一篇文章，finetuning 有一定帮助，但是不够细致。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/yihaizhiyan/article/details/41359957&quot;&gt;&lt;strong&gt;Dark Knowledge from Hinton&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;有心人整理的 Hinton 提到的 Dark Knowledge 的一些资源。&lt;/p&gt;

&lt;p&gt;[&lt;strong&gt;Stochastic Gradient Descent Tricks&lt;/strong&gt;][http://leon.bottou.org/publications/pdf/tricks-2012.pdf]&lt;/p&gt;

&lt;p&gt;L eon Bottou 写的 Stochastic Gradient Descent Tricks 挺好，做工程也要做的漂亮。 &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/zouxy09/article/details/45288129&quot;&gt;**神经网络训练中的Tricks之高效BP（反向传播算法）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;翻译文章。神经网络训练中的Tricks之高效BP（反向传播算法），来自与于《Neural Networks: Tricks of the Trade》一书第二版中的第一章 Efficient BackProp 的部分小节。&lt;/p&gt;

</description>
        
        <pubDate>Fri, 17 Apr 2015 00:00:00 +0800</pubDate>
        <link>/machinelearning/2015/04/17/collections-of-tips-for-machine-learning.html</link>
        <guid isPermaLink="true">/machinelearning/2015/04/17/collections-of-tips-for-machine-learning.html</guid>
      </item>
    
      <item>
        <title>Speed of Mini-Batch SGD</title>
                
        
          <description>&lt;p&gt;This post comes from a friend’s question, that he says sometimes mini-batch SGD converges more slowly than single SGD. &lt;/p&gt;

&lt;p&gt;Let’s begin with what these two kinds of method are and where they differ. Here notice that mini-batch methods come from batch methods.&lt;/p&gt;

&lt;h2 id=&quot;batch-gradient-descent&quot;&gt;Batch gradient descent&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/sgd_batch.png&quot; alt=&quot;figures from L´eon Bottou&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Batch gradient descent&lt;/strong&gt; computes the gradient using the whole dataset, while Stochastic gradient descent (SGD) computes the gradient using a single sample. This is great for convex, or relatively smooth error manifolds. In this case, we move &lt;em&gt;directly&lt;/em&gt; towards an optimum solution, either local or global. &lt;/p&gt;

&lt;h3 id=&quot;pros&quot;&gt;pros&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Great for convex, or relatively smooth error manifolds because it &lt;em&gt;directly&lt;/em&gt; towards to the optimum solution.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;cons&quot;&gt;cons&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Using the whole dataset means that it is updating the parameters using all the data. Each iteration of the batch gradient descent involves a computation of the average of the gradients of the loss function over the entire training data set. So the computation cost matters.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;stochastic-gradient-descent&quot;&gt;Stochastic gradient descent&lt;/h2&gt;

&lt;p&gt;While Batch gradient descent computes the gradient using the whole dataset, &lt;strong&gt;Stochastic gradient descent (SGD)&lt;/strong&gt; computes the gradient using a single sample. &lt;/p&gt;

&lt;h3 id=&quot;pros-1&quot;&gt;pros&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Obviously SGD’s computationally a whole lot faster. &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Single SGD works well &lt;strong&gt;better than&lt;/strong&gt; batch gradient descent &lt;em&gt;when the error manifolds that have lots of local maxima/minima&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;cons-1&quot;&gt;cons&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Sometimes, with the computational advantage, it should perform many more iterations of SGD, making many more steps than conventional batch gradient descent. &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;mini-batch-sgd&quot;&gt;mini-batch SGD&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/sgd_minibatch.png&quot; alt=&quot;figures from L´eon Bottou&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There comes the compromise of this two kinds of methods. When the batch size is 1, it is called stochastic gradient descent (GD).
When you set the batch size to 10 or to some extend larger, this method is called &lt;strong&gt;mini-batch SGD&lt;/strong&gt;. Mini-batch performs better than true stochastic gradient descent because when the gradient computed at each step uses more training examples, mini-batches tend to average a little of the noise out that single samples inherently bring. Thus, the amount of noise is reduced when using mini-batches. Therefore, we usually see smoother convergence out of local minima into a more optimal region. &lt;/p&gt;

&lt;p&gt;Thus, the batch size matters for the balance. We primally want the size to be small enough to avoid some of the poor local minima, and large enough that it doesn’t avoid the global minima or better-performing local minima. Also, a pratical consideratio raises from tractability that each sample or batch of samples must be loaded in a RAM-friendly size.&lt;/p&gt;

&lt;p&gt;So let’s be more clear:&lt;/p&gt;

&lt;h2 id=&quot;why-should-we-use-mini-batch&quot;&gt;Why should we use mini-batch?&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;It is small enough to let us implement vectorization in RAM.&lt;/li&gt;
  &lt;li&gt;Vectorization brings efficiency.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;disadvantage-of-mini-batch-sgd&quot;&gt;Disadvantage of mini-batch SGD&lt;/h2&gt;
&lt;p&gt;is the difficulty in balancing the batch size &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;. &lt;/p&gt;

&lt;p&gt;However, in the paper &lt;a href=&quot;http://link.springer.com/article/10.1007%2Fs10107-012-0572-5&quot;&gt;&lt;em&gt;Sample size selection in optimization methods for machine learning&lt;/em&gt;&lt;/a&gt;, the author points out that though large mini-batches are preferable to reduce the
communication cost, they may slow down convergence rate in practice. And Mu Li in this &lt;a href=&quot;http://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf&quot;&gt;&lt;em&gt;papar&lt;/em&gt;&lt;/a&gt; is dealing with this problem.&lt;/p&gt;

&lt;h2 id=&quot;ref&quot;&gt;Ref&lt;/h2&gt;

&lt;p&gt;[1]Bottou, Léon. &lt;em&gt;Large-scale machine learning with stochastic gradient descent.&lt;/em&gt; Proceedings of COMPSTAT’2010. Physica-Verlag HD, 2010. 177-186.&lt;/p&gt;

&lt;p&gt;[2]Bottou, Léon. &lt;em&gt;Online learning and stochastic approximations.&lt;/em&gt; On-line learning in neural networks 17.9 (1998): 142.&lt;/p&gt;

&lt;p&gt;[3]Li, Mu, et al. &lt;em&gt;Efficient mini-batch training for stochastic optimization.&lt;/em&gt; Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014.&lt;/p&gt;

</description>
        
        <pubDate>Sat, 11 Apr 2015 00:00:00 +0800</pubDate>
        <link>/papers/2015/04/11/speed-of-mini-batch-sgd.html</link>
        <guid isPermaLink="true">/papers/2015/04/11/speed-of-mini-batch-sgd.html</guid>
      </item>
    
  </channel>
</rss>