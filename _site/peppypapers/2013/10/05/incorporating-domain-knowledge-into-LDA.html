<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Incorpating Domain Knowledge into LDA &#8211; Yanran's Attic</title>
<meta name="description" content="Natural Language Processing, Semantic Embedding, Machine learning, deep learning">
<meta name="keywords" content="paper, domain knowledge, dirichlet forest, LDA">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Incorpating Domain Knowledge into LDA">
<meta property="og:description" content="Natural Language Processing, Semantic Embedding, Machine learning, deep learning">
<meta property="og:url" content="http://yanran.li/peppypapers/2013/10/05/incorporating-domain-knowledge-into-LDA.html">
<meta property="og:site_name" content="Yanran's Attic">





<link rel="canonical" href="http://yanran.li/peppypapers/2013/10/05/incorporating-domain-knowledge-into-LDA.html">
<link href="http://yanran.li/feed.xml" type="application/atom+xml" rel="alternate" title="Yanran's Attic Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://yanran.li/assets/css/main.min.css">
<!-- Webfonts -->
<link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://yanran.li/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://yanran.li/images/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://yanran.li/images/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://yanran.li/images/favicon.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://yanran.li/images/favicon.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://yanran.li/images/favicon.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://yanran.li/images/favicon.png">
<link rel="shortcut icon" href="/images/favicon.ico"/>
<link rel="bookmark" href="/images/favicon.ico"/>



<!-- MathJax -->
<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body id="post" >

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://yanran.li">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://yanran.li/images/avatar.jpg" alt="Yanran Li photo" class="author-photo">
					<h4>Yanran Li</h4>
					<p>Research Assistant at PolyU</p>
				</li>
				<li><a href="http://yanran.li/about/">Learn More</a></li>
				<li>
					<a href="mailto:yanranli.summer@gmail.com"><i class="icon-envelope"></i> Email</a>
				</li>
				
				
				<li>
					<a href="http://weibo.com/summerrlee"><i class="icon-weibo"></i> Weibo</a>
				</li>
				
				<li>
					<a href="https://www.linkedin.com/profile/view?id=233043238"><i class="icon-linkedin"></i> LinkedIn</a>
				</li>
				<li>
					<a href="http://github.com/niangaotuantuan"><i class="icon-github-alt"></i> GitHub</a>
				</li>
				
				
				
				
<!-- 				 -->
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://yanran.li/posts/">All Posts</a></li>
				<li><a href="http://yanran.li/categories/">All Categories</a></li>			
				<li><a href="http://yanran.li/tags/">All Tags</a></li>			
			</ul>
		</li>
		<li><a href="http://yanran.li/categories/#peppypapers">PaperNotes</a></li><li><a href="http://yanran.li/guestbook">GuestBook</a></li><li><a href="http://yanran.li/friends">Friends</a></li><li><a href="https://web.cs.dal.ca/~vlado/nlp/" class="external">NLP links</a></li><li><a href="http://www.reddit.com/r/machinelearning" class="external">r/machinelearning</a></li><li><a href="https://plus.google.com/communities/112866381580457264725" class="external">Deep Learning G+</a></li>
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->




<div id="main" role="main">
  <article class="hentry">
    <header class="header-title">
      <div class="header-title-wrap">
        
          <h1 class="entry-title"><a href="http://yanran.li/peppypapers/2013/10/05/incorporating-domain-knowledge-into-LDA.html" rel="bookmark" title="Incorpating Domain Knowledge into LDA">Incorpating Domain Knowledge into LDA</a></h1>
        
        <h2>October 05, 2013</h2>
      </div><!-- /.header-title-wrap -->
    </header>
    <div class="entry-content">
      <p>Recent years, there has been emerging research on knowledge-based models and methods. Researchers have tried in various ways to express/embed/structure the knowledge and then incorporating them into some existing models, among which is LDA (Latent Dirichlet Allocation). </p>

<p>For further detailed about LDA, please investigate through [Blei el al., 2003].  The basic idea and foundation of LDA is handling <strong>word co-occurrence</strong> pattern to discover the latent semantic meaning. The simple model has limited resolution to deeper latent sementics and thus the variations of LDA are bursting. One focus to expand LDA is how to incorporating more <strong>prior knowledge</strong> into it.</p>

<h2 id="types-of-prior-knowledge">Types of Prior Knowledge</h2>

<p>Basically, all types of knowledge incorporation is to change the prior distribution of Dirichlet setting in LDA. </p>

<p>a.	在传统 LDA 里，有两组先验，一种是文档~主题的先验，来自于一个对称的<script type="math/tex"> \mathcal Dir(\alpha) </script>；一种是主题~词汇的先验，来自于一个对称的<script type="math/tex"> \mathcal Dir(\beta) </script> ——都是 symmetric Dirichlet Distribution。所以按理，可以把这两种先验分别改成不对称的——这样就加入了更多的 knowledge 信息。</p>

<p>b.	在 Rethinking LDA 里一文<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>中，结合两种先验与两种不同的先验设定方法，可以得到以下四种组合：</p>

<ul>
  <li>AA：文档~主题分布和主题~词汇分布都采用非对称；</li>
  <li>AS：文档~主题分布采用非对称的先验，而主题~词汇分布采用对称的先验；</li>
  <li>SA：文档~主题分布采用对称的先验，而主题~词汇采用非对称；</li>
  <li>SS：文档~主题分布和主题~词汇分布都采用对称的先验。</li>
</ul>

<p>他们的实验发现其中 AS 的方法可以更高地提高 LDA 对于文本建模的能力。</p>

<p>c.  典型的打破“对称”文档~主题分布先验（AS）的几个model，有很好理解的 Twitter-LDA，也有 Behavior-LDA。同时，supervised-LDA 也可以看做一个非结构化的打破先验的方式，变形后有 SeededLDA（在两个层次的先验都通过设计加入了不对称信息）。</p>

<p>d.	除了通过<strong>直接</strong>地改变概率分布来加入先验的方法，这几年来开始有越来越多的研究者想将结构化的先验知识加入 LDA 。这种结构化的先验，不再是简单的 prior distribution，更可以倾向于称为“knowledge”。这样的研究之所以盛行，一方面是长期以来的结构化知识库已有很多（且因为还要继续建立知识图谱等，结构化仍将是未来的趋势），另一方面形式语言（逻辑语言）的表示的研究一直都没有停止。这种结构化的引入 knowledge 的方法，本质也是通过打破先验设定的 symmetric Dirichlet Distribution。下文将重点总结这方面的工作。</p>

<h2 id="domain-dependent-model">Domain-dependent Model：</h2>
<ul>
  <li>
    <p>CIKM’13 里，Zhiyuan Chen（也在 Bing Liu那里）的一篇 Discovering Coherent Topics<sup id="fnref:3"><a href="#fn:3" class="footnote">2</a></sup> 里将 incorporating knowledge 的研究分成了 domain-dependent 的和 domain-independent：前者是 expert 知道（普通人不一定熟悉，需要 expert 来参与编辑）的知识而且有知识领域限制，后者是各领域通用的一些知识。</p>
  </li>
  <li>
    <p>同样是上述文章，提到了<sup id="fnref:2"><a href="#fn:2" class="footnote">3</a></sup>,<sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup>,<sup id="fnref:5"><a href="#fn:5" class="footnote">5</a></sup>,<sup id="fnref:6"><a href="#fn:6" class="footnote">6</a></sup>,<sup id="fnref:7"><a href="#fn:7" class="footnote">7</a></sup> 的论文都是 domain-dependent knowledge-based 的。</p>
  </li>
  <li>
    <p>其中，Dirichlet Forest<sup id="fnref:2:1"><a href="#fn:2" class="footnote">3</a></sup> 和 Jerry Zhu 的 First-Order Logic<sup id="fnref:4:1"><a href="#fn:4" class="footnote">4</a></sup> 的形式化加入 domain-knowledge 的方法还是比较有代表性。前者是将领域内一定会一起出现（两个词的出现概率都很大或者都很小）的词和一定不能一起出现的词分别表示为 Must-Link 和 Cannot-Link，然后表示成树中的结点和结点之间的连接关系。但这个 Link 关系是可传递的，所以会导致“错误”的先验知识加入（CIKM’13 中提到了这点）。         </p>
  </li>
</ul>

<h2 id="domain-independent-model">Domain-independent Model：</h2>

<ul>
  <li>
    <p>按照 Zhiyuan Chen 的说法，他们在 CIKM’13 里提出的 GK-LDA<sup id="fnref:3:1"><a href="#fn:3" class="footnote">2</a></sup> 应该是第一个 domain-independent model。所以这个部分只谈他们的那篇论文（GK-LDA 是 General Knowledge 的缩写，即 domain-independent 的 knowledge）。</p>
  </li>
  <li>
    <p>在这篇论文里，他们的假设是，<em>there is a vast amount of available in online dictionaries or other resources that can be exploited in a model to generate more coherent topic</em>. 而通过 extract，就可以把这样的 lexical knowledge 提取成 a general knowledge base.</p>
  </li>
  <li>
    <p>他们采取的知识表达结构是 lexical relationships on words. 简称 <strong>LR-sets</strong>。LR-sets 有很多种关系，比如同义词、反义词，这篇文章中重点讲的是 adjective-attribute 这种 relationship，e.g. (expensive-price).</p>
  </li>
  <li>
    <p>他们提出的 GK-LDA 依然是 一种 LDA 的变形，而且是基于他们组再之前的工作——IJCAI’13 的 Leveraging Multi-Domain Prior Knowledge<sup id="fnref:8"><a href="#fn:8" class="footnote">8</a></sup> 里的 MDK-LDA。</p>
  </li>
</ul>

<h2 id="mdk-lda-b--mdk-lda--gk-lda">从 MDK-LDA (b) 到 MDK-LDA 到 GK-LDA：</h2>

<ul>
  <li>
    <p>主要总结 Zhiyuan Chen 的两篇工作，之前提过的 MDK-LDA 和 GK-LDA。</p>
  </li>
  <li>
    <p>MDK-LDA 是 multi-domain knowledge 的缩写，从思想上来看是一种 Transfer Learning 的想法，prior from other domain can help topic model in new domain.</p>
  </li>
  <li>
    <p>所谓的 multi-domain 可以通过 <strong>s-set</strong> 表示，比如 “light” has 2 s-set {light, heavy, weight} 和 {light, bright, luminanee}，表示出了 light 的两个词义。那么这个工作就是去 leverage 这个 s-sets。</p>
  </li>
  <li>
    <p>他们在 IJCAI’13 的那篇里<sup id="fnref:8:1"><a href="#fn:8" class="footnote">8</a></sup> 主要分析了 之前的 domain-knowledge 会遇到的两个大问题，MDK-LDA 解决了其中一个 adverse effect 的问题，而 GK-LDA 两个都解决了（还有一个是错误先验知识带来的问题）。MDK-LDA 解决的主要在 LDA 问题里，如何使得一些少见的词但是确实是同一个 set 里的词的低频不会影响 topic modeling 的学习（不仅仅用 TF-IDF 消除影响），那么他们认为 <em>the words in an s-set share a similar semantic meaning in the model should redistribute the probability masses over words in the s-set to ensure that they have similar probability under the same topic</em>. 这个思想使得他们在 MDK-LDA(basic) 之上加入了 GPU <sup id="fnref:9"><a href="#fn:9" class="footnote">9</a></sup>：像抽出小球再放回同颜色的球的思想一样，去改变同一个 s-set 里的 word 的dist.</p>
  </li>
  <li>
    <p>在 MDK-LDA 之上，解决第二个问题的就是 GK-LDA，也就是在 CIKM’13 里的那篇<sup id="fnref:3:2"><a href="#fn:3" class="footnote">2</a></sup>。MDK-LDA 没法避免当我们的先验 s-set 是错误的（这也是其他许多 domain-dependent model 的问题，必须保证我们的先验知识都是正确的）对 performance 的影响。 GK-LDA 加入了一个 word correlation matrix 的计算 和 加入一个 threshold，减少了 wrong LR-set 的的影响。</p>
  </li>
  <li>
    <p>其中加入 GPU 的思想，和 CRP 中如何改变人坐在具体某个餐桌的概率的思想是一致的（只是一个模型的不同解释）。</p>
  </li>
</ul>

<h2 id="footnotes">Footnotes</h2>

<ul>
  <li>
    <p>Transfer Learning 和 Active Learning、Online Learning 等等都有关系。这部分内容还没有系统学习过，之前一篇<a href="http://yanran.li/2013/07/covariate-shift-correction/">文章</a>也有提到这里的一个小坑。</p>
  </li>
  <li>
    <p>GPU<sup id="fnref:9:1"><a href="#fn:9" class="footnote">9</a></sup>，是 Generalized Polya Urn 的简称。搞懂 LDA 必须先学习的模型。将这个过程generalized, 可以推向Polya Urn’s Process。Polya Urn’s Model 是比较直观的理解 Dirichlet Process 的一种解释模型。模型中抽出球再放回就是对当前的多项分布进行抽样（同时不改变该分布），又放回一个同样的球就是依当前多项分布产生新的多项分布。假设从<script type="math/tex"> \mathcal Dir(\alpha, K) </script>中抽样，那么新产生的多项分布共有 K 个，其概率质量与当前多项分布成比例。K 个新产生的多项分布的加权平均与原多项分布是同分布的。而在之前的 CIKM’13 论文<sup id="fnref:3:3"><a href="#fn:3" class="footnote">2</a></sup>中就是通过改变每次放回的“球”（LR-set 里同一个 set 的词）的“颜色”和数量来改变 prior knowledge 的。这种思想感觉还是很赞的。</p>
  </li>
</ul>

<h2 id="reference">Reference</h2>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Hanna Wallach, David Mimno and Andrew McCallum. Rethinking LDA: Why Priors Matter. NIPS, 2009, Vancouver, BC. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun Hsu, Malu Castellanos, and Riddhiman Ghosh. Discovering Coherent Topics using General Knowledge. Proceedings of the ACM Conference of Information and Knowledge Management (CIKM’13). October 27 - November1, Burlingame, CA, USA. <a href="#fnref:3" class="reversefootnote">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote">&#8617;<sup>2</sup></a> <a href="#fnref:3:2" class="reversefootnote">&#8617;<sup>3</sup></a> <a href="#fnref:3:3" class="reversefootnote">&#8617;<sup>4</sup></a></p>
    </li>
    <li id="fn:2">
      <p>Andrzejewski, D., Zhu, X. and Craven, M. 2009. Incorporating domain knowledge into topic modeling via Dirichlet Forest priors. ICML, 25–32. <a href="#fnref:2" class="reversefootnote">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:4">
      <p>Andrzejewski, D., Zhu, X., Craven, M. and Recht, B. 2011. A framework for incorporating general domain knowledge into latent Dirichlet allocation using first-order logic. IJCAI, 1171–1177. <a href="#fnref:4" class="reversefootnote">&#8617;</a> <a href="#fnref:4:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:5">
      <p>Burns, N., Bi, Y., Wang, H. and Anderson, T. 2012. Extended Twofold-LDA Model for Two Aspects in One Sentence. Advances in Computational Intelligence. Springer Berlin Heidelberg. 265–275. <a href="#fnref:5" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Jagarlamudi, J., III, H.D. and Udupa, R. 2012. Incorporating Lexical Priors into Topic Models. EACL, 204–213 <a href="#fnref:6" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>Mukherjee, A. and Liu, B. 2012. Aspect Extraction through SemiSupervised Modeling. ACL, 339–348. <a href="#fnref:7" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:8">
      <p>Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun Hsu, Malu Castellanos, and Riddhiman Ghosh. Leveraging Multi-Domain Prior Knowledge in Topic Models. Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI’13). August 3-9, 2013, Beijing, China. <a href="#fnref:8" class="reversefootnote">&#8617;</a> <a href="#fnref:8:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:9">
      <p>David Mimno, Hanna Wallach, Edmund Talley, Miriam Leenders, Andrew McCallum. Optimizing Semantic Coherence in Topic Models. EMNLP (2011). <a href="#fnref:9" class="reversefootnote">&#8617;</a> <a href="#fnref:9:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>

      <footer class="entry-meta">
        <span class="entry-tags"><a href="http://yanran.li/tags/#paper" title="Pages tagged paper" class="tag">paper</a><a href="http://yanran.li/tags/#domain knowledge" title="Pages tagged domain knowledge" class="tag">domain knowledge</a><a href="http://yanran.li/tags/#dirichlet forest" title="Pages tagged dirichlet forest" class="tag">dirichlet forest</a><a href="http://yanran.li/tags/#LDA" title="Pages tagged LDA" class="tag">LDA</a></span>
        <span><a href="http://yanran.li/peppypapers/2013/10/05/incorporating-domain-knowledge-into-LDA.html" rel="bookmark" title="Incorpating Domain Knowledge into LDA">Incorpating Domain Knowledge into LDA</a> was published on <span class="entry-date date published updated"><time datetime="2013-10-05T00:00:00+08:00">October 05, 2013</time></span></span>
        
        <span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>
        <div class="social-share">
          <ul class="socialcount socialcount-small inline-list">
            <li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=http://yanran.li/peppypapers/2013/10/05/incorporating-domain-knowledge-into-LDA.html" title="Share on Facebook"><span class="count"><i class="icon-facebook-sign"></i> Like</span></a></li>
            <li class="twitter"><a href="https://twitter.com/intent/tweet?text=http://yanran.li/peppypapers/2013/10/05/incorporating-domain-knowledge-into-LDA.html" title="Share on Twitter"><span class="count"><i class="icon-twitter-sign"></i> Tweet</span></a></li>
            <li class="googleplus"><a href="https://plus.google.com/share?url=http://yanran.li/peppypapers/2013/10/05/incorporating-domain-knowledge-into-LDA.html" title="Share on Google Plus"><span class="count"><i class="icon-google-plus-sign"></i> +1</span></a></li>
          </ul>
        </div><!-- /.social-share -->
      </footer>
    </div><!-- /.entry-content -->
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
    
    <div class="read-more">
      
        <div class="read-more-header">
          <a href="http://yanran.li/r/2013/10/04/loading-big-data-in-R.html" class="read-more-btn">Read More</a>
        </div><!-- /.read-more-header -->
        <div class="read-more-content">
          <h3><a href="http://yanran.li/peppypapers/2015/04/18/DeepWalk-Online-Learning-of-Social-Representations.html" title="DeepWalk Online Learning of Social Representations">DeepWalk Online Learning of Social Representations</a></h3>
          <p>这是一篇我个人非常喜欢的论文，不仅提出了一个想法，更展示了这个想法的可行性和可能空间。提出的想法是利用网络结构信息将用户表示为低维实值向量，学出来的表示是最重要的，因为有了表示就可以用来加在许多其他任务上。 <a href="http://yanran.li/peppypapers/2015/04/18/DeepWalk-Online-Learning-of-Social-Representations.html">Continue reading</a></p>
        </div><!-- /.read-more-content -->
      
      <div class="read-more-list">
        
          <div class="list-item">
            <h4><a href="http://yanran.li/machinelearning/2015/04/17/collections-of-tips-for-machine-learning.html" title="Collections of Tips for Machine Learning">Collections of Tips for Machine Learning</a></h4>
            <span>Published on April 17, 2015</span>
          </div><!-- /.list-item -->
        
          <div class="list-item">
            <h4><a href="http://yanran.li/life/2015/04/15/how-i-manage-my-knowledge.html" title="我是这样进行知识管理的">我是这样进行知识管理的</a></h4>
            <span>Published on April 15, 2015</span>
          </div><!-- /.list-item -->
        
      </div><!-- /.read-more-list -->
      
    </div><!-- /.read-more -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2015 Yanran Li. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://yanran.li/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://yanran.li/assets/js/scripts.min.js"></script>

<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'yanranli'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>	        

</body>
</html>
