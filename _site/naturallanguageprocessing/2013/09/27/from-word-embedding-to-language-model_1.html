<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>From Word Embedding to Language Model(1) &#8211; Yanran's Attic</title>
<meta name="description" content="Natural Language Processing, Semantic Embedding, Machine learning, deep learning">
<meta name="keywords" content="word embedding, deep learning, language model">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="From Word Embedding to Language Model(1)">
<meta property="og:description" content="Natural Language Processing, Semantic Embedding, Machine learning, deep learning">
<meta property="og:url" content="http://yanran.li/naturallanguageprocessing/2013/09/27/from-word-embedding-to-language-model_1.html">
<meta property="og:site_name" content="Yanran's Attic">





<link rel="canonical" href="http://yanran.li/naturallanguageprocessing/2013/09/27/from-word-embedding-to-language-model_1.html">
<link href="http://yanran.li/feed.xml" type="application/atom+xml" rel="alternate" title="Yanran's Attic Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://yanran.li/assets/css/main.min.css">
<!-- Webfonts -->
<link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://yanran.li/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://yanran.li/images/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://yanran.li/images/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://yanran.li/images/favicon.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://yanran.li/images/favicon.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://yanran.li/images/favicon.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://yanran.li/images/favicon.png">
<link rel="shortcut icon" href="/images/favicon.ico"/>
<link rel="bookmark" href="/images/favicon.ico"/>



<!-- MathJax -->
<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body id="post" >

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://yanran.li">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://yanran.li/images/avatar.jpg" alt="Yanran Li photo" class="author-photo">
					<h4>Yanran Li</h4>
					<p>Research Assistant at PolyU</p>
				</li>
				<li><a href="http://yanran.li/about/">Learn More</a></li>
				<li>
					<a href="mailto:yanranli.summer@gmail.com"><i class="icon-envelope"></i> Email</a>
				</li>
				
				
				<li>
					<a href="http://weibo.com/summerrlee"><i class="icon-weibo"></i> Weibo</a>
				</li>
				
				<li>
					<a href="https://www.linkedin.com/profile/view?id=233043238"><i class="icon-linkedin"></i> LinkedIn</a>
				</li>
				<li>
					<a href="http://github.com/niangaotuantuan"><i class="icon-github-alt"></i> GitHub</a>
				</li>
				
				
				
				
<!-- 				 -->
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://yanran.li/posts/">All Posts</a></li>
				<li><a href="http://yanran.li/categories/">All Categories</a></li>			
				<li><a href="http://yanran.li/tags/">All Tags</a></li>			
			</ul>
		</li>
		<li><a href="http://yanran.li/categories/#peppypapers">PaperNotes</a></li><li><a href="http://yanran.li/guestbook">GuestBook</a></li><li><a href="http://yanran.li/friends">Friends</a></li><li><a href="https://web.cs.dal.ca/~vlado/nlp/" class="external">NLP links</a></li><li><a href="http://www.reddit.com/r/machinelearning" class="external">r/machinelearning</a></li><li><a href="https://plus.google.com/communities/112866381580457264725" class="external">Deep Learning G+</a></li>
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->




<div id="main" role="main">
  <article class="hentry">
    <header class="header-title">
      <div class="header-title-wrap">
        
          <h1 class="entry-title"><a href="http://yanran.li/naturallanguageprocessing/2013/09/27/from-word-embedding-to-language-model_1.html" rel="bookmark" title="From Word Embedding to Language Model(1)">From Word Embedding to Language Model(1)</a></h1>
        
        <h2>September 27, 2013</h2>
      </div><!-- /.header-title-wrap -->
    </header>
    <div class="entry-content">
      <h2 id="deep-learning-basement">Deep Learning Basement</h2>

<p>There are some basic important concepts in deep learning. </p>

<h3 id="backpropagation-network">Backpropagation Network</h3>

<p>BP Network is a special case of <strong>Feedforward networks</strong>, constituted by <strong>nonlinear continuous transformation</strong> units, and adjusted by <strong>error back propagation algorithm</strong>. The BP algorithm can be divided into two phases: propagation and weight update.</p>

<p>Also, some techniques are used to improve BP algorithm. For instance, gentic algorithm (supervised) and RBM (unsupervised), Auto-decoder (Deep Learning) can be in order to search for good weights before training.</p>

<h3 id="distributed-representations">Distributed Representations</h3>

<p>In contrast to the “atomic” or “localist” representations employed in traditional cognitive science, a distributed representation is one in which <strong>“each entity is represented by a pattern of activity distributed over many computing elements, and each computing element is involved in representing many different entities.”</strong></p>

<p>Yoshua Bengio gave a vivid analogy between “local” and “distributed” in his given talk, <em>Learning to Represent Semantics</em>.</p>

<p><img src="http://i.imgur.com/b8sEQFd.png" alt="" /></p>

<h2 id="deep-learning-motivation-for-semantics">Deep Learning Motivation for Semantics</h2>

<p>In Language Model, given a probability for a longer word sequence, 
<script type="math/tex"> P(w_{1},...,w_{l})=\prod_{t}P(w_{t}|w{t-1},...,w_{t-n+1}) </script>, 
we then predict P(next word|context). And in traditional n-gram Language Model, we use counts and smoothing to calculate the conditional probability. </p>

<p>However, the traditional n-gram LM fails with <em>Curse of dimensionality</em>: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training.</p>

<p>For example, the training sentence:</p>

<pre><code>The cat is walking in the bedroom.
</code></pre>

<p>The Test sentence:</p>

<pre><code>A dog was running in a room.
</code></pre>

<p>Under the sparsity/curse of dim. problem, we seek for a <strong>similar representations for semantically similar phrases</strong>.</p>

<h3 id="word-embedding-for-representing-words">Word Embedding for Representing Words</h3>

<p><strong>Word Embedding</strong> gives a low dimension (usually 50) distributed representation (Hinton, 1986) for each word. Thus similar words have similar representations. This distributed continuous-valued vector for each word can be learned from raw text (Colobert &amp; Weston, 2008).</p>

<h3 id="composing-words-with-nn">Composing Words with NN</h3>

<p>The Word Embedding is only the representation for each word, we should learn how to compose words into phrases and semantic relations. And Theorems on advantage of depth  (Hastad et al 86 &amp; 91, Bengio et al 2007, Bengio &amp;
Delalleau 2011, Braverman 2011) proves that deep architectures are more expressive and sharing components exponentially strengthens the advantage.</p>

<h2 id="neural-network-language-model">Neural Network Language Model</h2>

<p>Training the word embedding and the neural network at the same time, there are two types of system: Neural network language model and the others. The former type consists of (1)input: context; (2)output: distribution of next word, <script type="math/tex">\vert V\vert</script> nodes. And the latter type (1)input: entire sequence; (2)output: score, 1 node.</p>

<h3 id="probabilistic-neural-language-model">Probabilistic Neural Language Model</h3>

<table>
  <tbody>
    <tr>
      <td>Back to the problem, predict P(next word</td>
      <td>context). To calcute the conditional probability:</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Traiditional N-gram Languge Model uses counts and smoothing.</li>
  <li>Probabilistic Neural Language Model uses word embedding and neural network.</li>
</ul>

<p>After building the language model, compute word vectors during this process:</p>

<script type="math/tex; mode=display"> f(i,w_{t-1},...,w_{t-n+1})=g(i,C(w_{t-1}),...,C(w_{t-n+1})) </script>

<p><img src="http://i.imgur.com/gLGi5vU.png" alt="" /></p>

<p>This structure is Bengio’s groundbreaking work.
It has a linear projection layer, a nonlinear hidden layer and a softmax output layer. The sparse history h is projected into some continuous low-dimensional space, where similar histories get clustered. Moreover, the model is more robust: less parameters have to be estimated from the training data.</p>

<p>But the model has limitation below:</p>

<ul>
  <li>Complexity: <script type="math/tex"> (n × m) × h + h × \vert V\vert </script></li>
  <li>New words fails</li>
  <li>Long term context ignored</li>
  <li>Lack priori knowledge, such as POS, semantic information (WordNet)</li>
</ul>

<h2 id="problems-remain">Problems Remain</h2>

<p>How to design a neural network? An art or a science? :P</p>

<h2 id="references">References</h2>

<p>Yoshua Bengio. Learning to Represent Semantics. Words2Actions Workshop, NAACL HLT 2012, Montreal</p>

<p>Hinton, G. E., McClelland, J. L., and Rumelhart, D. E. (1986). Distributed representations. In Rumelhart, D. E. and McClelland, J. L., editors, Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations, MIT Press, Cambridge, MA.</p>

<p>R. Collobert and J. Weston. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In International Conference on Machine Learning, ICML, 2008.</p>

<p>Morin and Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS 2005.</p>

<p>Mnih and Hinton. A Scalable Hierarchical Distributed Language Model. NIPS 2008.</p>

      <footer class="entry-meta">
        <span class="entry-tags"><a href="http://yanran.li/tags/#word embedding" title="Pages tagged word embedding" class="tag">word embedding</a><a href="http://yanran.li/tags/#deep learning" title="Pages tagged deep learning" class="tag">deep learning</a><a href="http://yanran.li/tags/#language model" title="Pages tagged language model" class="tag">language model</a></span>
        <span><a href="http://yanran.li/naturallanguageprocessing/2013/09/27/from-word-embedding-to-language-model_1.html" rel="bookmark" title="From Word Embedding to Language Model(1)">From Word Embedding to Language Model(1)</a> was published on <span class="entry-date date published updated"><time datetime="2013-09-27T00:00:00+08:00">September 27, 2013</time></span></span>
        
        <span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>
        <div class="social-share">
          <ul class="socialcount socialcount-small inline-list">
            <li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=http://yanran.li/naturallanguageprocessing/2013/09/27/from-word-embedding-to-language-model_1.html" title="Share on Facebook"><span class="count"><i class="icon-facebook-sign"></i> Like</span></a></li>
            <li class="twitter"><a href="https://twitter.com/intent/tweet?text=http://yanran.li/naturallanguageprocessing/2013/09/27/from-word-embedding-to-language-model_1.html" title="Share on Twitter"><span class="count"><i class="icon-twitter-sign"></i> Tweet</span></a></li>
            <li class="googleplus"><a href="https://plus.google.com/share?url=http://yanran.li/naturallanguageprocessing/2013/09/27/from-word-embedding-to-language-model_1.html" title="Share on Google Plus"><span class="count"><i class="icon-google-plus-sign"></i> +1</span></a></li>
          </ul>
        </div><!-- /.social-share -->
      </footer>
    </div><!-- /.entry-content -->
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
    
    <div class="read-more">
      
        <div class="read-more-header">
          <a href="http://yanran.li/machinelearning/2013/07/14/basics-for-regularization.html" class="read-more-btn">Read More</a>
        </div><!-- /.read-more-header -->
        <div class="read-more-content">
          <h3><a href="http://yanran.li/peppypapers/2015/04/18/DeepWalk-Online-Learning-of-Social-Representations.html" title="DeepWalk Online Learning of Social Representations">DeepWalk Online Learning of Social Representations</a></h3>
          <p>这是一篇我个人非常喜欢的论文，不仅提出了一个想法，更展示了这个想法的可行性和可能空间。提出的想法是利用网络结构信息将用户表示为低维实值向量，学出来的表示是最重要的，因为有了表示就可以用来加在许多其他任务上。 <a href="http://yanran.li/peppypapers/2015/04/18/DeepWalk-Online-Learning-of-Social-Representations.html">Continue reading</a></p>
        </div><!-- /.read-more-content -->
      
      <div class="read-more-list">
        
          <div class="list-item">
            <h4><a href="http://yanran.li/machinelearning/2015/04/17/collections-of-tips-for-machine-learning.html" title="Collections of Tips for Machine Learning">Collections of Tips for Machine Learning</a></h4>
            <span>Published on April 17, 2015</span>
          </div><!-- /.list-item -->
        
          <div class="list-item">
            <h4><a href="http://yanran.li/life/2015/04/15/how-i-manage-my-knowledge.html" title="我是这样进行知识管理的">我是这样进行知识管理的</a></h4>
            <span>Published on April 15, 2015</span>
          </div><!-- /.list-item -->
        
      </div><!-- /.read-more-list -->
      
    </div><!-- /.read-more -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2015 Yanran Li. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://yanran.li/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://yanran.li/assets/js/scripts.min.js"></script>

<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'yanranli'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>	        

</body>
</html>
