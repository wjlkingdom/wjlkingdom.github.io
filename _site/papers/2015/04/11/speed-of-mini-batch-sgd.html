<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Speed of Mini-Batch SGD &#8211; Maggie's KingDom</title>
<meta name="description" content="Natural Language Processing, Machine learning, Deep learning">
<meta name="keywords" content="paper, SGD, convex optimization, neural networks, batch, machine learning">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Speed of Mini-Batch SGD">
<meta property="og:description" content="Natural Language Processing, Machine learning, Deep learning">
<meta property="og:url" content="/papers/2015/04/11/speed-of-mini-batch-sgd.html">
<meta property="og:site_name" content="Maggie's KingDom">





<link rel="canonical" href="/papers/2015/04/11/speed-of-mini-batch-sgd.html">
<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Maggie's KingDom Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.min.css">
<!-- Webfonts -->
<link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="/images/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="/images/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="/images/favicon.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="/images/favicon.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="/images/favicon.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/images/favicon.png">
<link rel="shortcut icon" href="/images/favicon.ico"/>
<link rel="bookmark" href="/images/favicon.ico"/>



<!-- MathJax -->
<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body id="post" >

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="/images/avatar.jpg" alt="Maggie photo" class="author-photo">
					<h4>Maggie</h4>
					<p>COMP at PolyU</p>
				</li>
				<li><a href="/about/">Learn More</a></li>
				<li>
					<a href="mailto:yanranli.summer@gmail.com"><i class="icon-envelope"></i> Email</a>
				</li>
				
				
				
				
				
				<li>
					<a href="http://github.com/wjlkingdom"><i class="icon-github-alt"></i> GitHub</a>
				</li>
				
				
				
				
<!-- 				 -->
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="/posts/">All Posts</a></li>
				<li><a href="/categories/">All Categories</a></li>			
				<li><a href="/tags/">All Tags</a></li>			
			</ul>
		</li>
		<li><a href="/categories/#resource">Resources</a></li><li><a href="/categories/#paper">Papers</a></li><li><a href="https://web.cs.dal.ca/~vlado/nlp/" class="external">NLP links</a></li><li><a href="http://www.reddit.com/r/machinelearning" class="external">r/machinelearning</a></li><li><a href="https://plus.google.com/communities/112866381580457264725" class="external">Deep Learning G+</a></li>
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->




<div id="main" role="main">
  <article class="hentry">
    <header class="header-title">
      <div class="header-title-wrap">
        
          <h1 class="entry-title"><a href="/papers/2015/04/11/speed-of-mini-batch-sgd.html" rel="bookmark" title="Speed of Mini-Batch SGD">Speed of Mini-Batch SGD</a></h1>
        
        <h2>April 11, 2015</h2>
      </div><!-- /.header-title-wrap -->
    </header>
    <div class="entry-content">
      <p>This post comes from a friend’s question, that he says sometimes mini-batch SGD converges more slowly than single SGD. </p>

<p>Let’s begin with what these two kinds of method are and where they differ. Here notice that mini-batch methods come from batch methods.</p>

<h2 id="batch-gradient-descent">Batch gradient descent</h2>

<p><img src="/images/sgd_batch.png" alt="figures from L´eon Bottou" /></p>

<p><strong>Batch gradient descent</strong> computes the gradient using the whole dataset, while Stochastic gradient descent (SGD) computes the gradient using a single sample. This is great for convex, or relatively smooth error manifolds. In this case, we move <em>directly</em> towards an optimum solution, either local or global. </p>

<h3 id="pros">pros</h3>

<ol>
  <li>Great for convex, or relatively smooth error manifolds because it <em>directly</em> towards to the optimum solution.</li>
</ol>

<h3 id="cons">cons</h3>

<ol>
  <li>Using the whole dataset means that it is updating the parameters using all the data. Each iteration of the batch gradient descent involves a computation of the average of the gradients of the loss function over the entire training data set. So the computation cost matters.</li>
</ol>

<h2 id="stochastic-gradient-descent">Stochastic gradient descent</h2>

<p>While Batch gradient descent computes the gradient using the whole dataset, <strong>Stochastic gradient descent (SGD)</strong> computes the gradient using a single sample. </p>

<h3 id="pros-1">pros</h3>

<ol>
  <li>
    <p>Obviously SGD’s computationally a whole lot faster. </p>
  </li>
  <li>
    <p>Single SGD works well <strong>better than</strong> batch gradient descent <em>when the error manifolds that have lots of local maxima/minima</em>.</p>
  </li>
</ol>

<h3 id="cons-1">cons</h3>

<ol>
  <li>Sometimes, with the computational advantage, it should perform many more iterations of SGD, making many more steps than conventional batch gradient descent. </li>
</ol>

<h2 id="mini-batch-sgd">mini-batch SGD</h2>

<p><img src="/images/sgd_minibatch.png" alt="figures from L´eon Bottou" /></p>

<p>There comes the compromise of this two kinds of methods. When the batch size is 1, it is called stochastic gradient descent (GD).
When you set the batch size to 10 or to some extend larger, this method is called <strong>mini-batch SGD</strong>. Mini-batch performs better than true stochastic gradient descent because when the gradient computed at each step uses more training examples, mini-batches tend to average a little of the noise out that single samples inherently bring. Thus, the amount of noise is reduced when using mini-batches. Therefore, we usually see smoother convergence out of local minima into a more optimal region. </p>

<p>Thus, the batch size matters for the balance. We primally want the size to be small enough to avoid some of the poor local minima, and large enough that it doesn’t avoid the global minima or better-performing local minima. Also, a pratical consideratio raises from tractability that each sample or batch of samples must be loaded in a RAM-friendly size.</p>

<p>So let’s be more clear:</p>

<h2 id="why-should-we-use-mini-batch">Why should we use mini-batch?</h2>

<ol>
  <li>It is small enough to let us implement vectorization in RAM.</li>
  <li>Vectorization brings efficiency.</li>
</ol>

<h2 id="disadvantage-of-mini-batch-sgd">Disadvantage of mini-batch SGD</h2>
<p>is the difficulty in balancing the batch size <script type="math/tex">b</script>. </p>

<p>However, in the paper <a href="http://link.springer.com/article/10.1007%2Fs10107-012-0572-5"><em>Sample size selection in optimization methods for machine learning</em></a>, the author points out that though large mini-batches are preferable to reduce the
communication cost, they may slow down convergence rate in practice. And Mu Li in this <a href="http://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf"><em>papar</em></a> is dealing with this problem.</p>

<h2 id="ref">Ref</h2>

<p>[1]Bottou, Léon. <em>Large-scale machine learning with stochastic gradient descent.</em> Proceedings of COMPSTAT’2010. Physica-Verlag HD, 2010. 177-186.</p>

<p>[2]Bottou, Léon. <em>Online learning and stochastic approximations.</em> On-line learning in neural networks 17.9 (1998): 142.</p>

<p>[3]Li, Mu, et al. <em>Efficient mini-batch training for stochastic optimization.</em> Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014.</p>


      <footer class="entry-meta">
        <span class="entry-tags"><a href="/tags/#paper" title="Pages tagged paper" class="tag">paper</a><a href="/tags/#SGD" title="Pages tagged SGD" class="tag">SGD</a><a href="/tags/#convex optimization" title="Pages tagged convex optimization" class="tag">convex optimization</a><a href="/tags/#neural networks" title="Pages tagged neural networks" class="tag">neural networks</a><a href="/tags/#batch" title="Pages tagged batch" class="tag">batch</a><a href="/tags/#machine learning" title="Pages tagged machine learning" class="tag">machine learning</a></span>
        <span><a href="/papers/2015/04/11/speed-of-mini-batch-sgd.html" rel="bookmark" title="Speed of Mini-Batch SGD">Speed of Mini-Batch SGD</a> was published on <span class="entry-date date published updated"><time datetime="2015-04-11T00:00:00+08:00">April 11, 2015</time></span></span>
        
        <span class="author vcard"><span class="fn"><a href="/about/" title="About Maggie">Maggie</a></span></span>
        <div class="social-share">
          <ul class="socialcount socialcount-small inline-list">
            <li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=/papers/2015/04/11/speed-of-mini-batch-sgd.html" title="Share on Facebook"><span class="count"><i class="icon-facebook-sign"></i> Like</span></a></li>
            <li class="twitter"><a href="https://twitter.com/intent/tweet?text=/papers/2015/04/11/speed-of-mini-batch-sgd.html" title="Share on Twitter"><span class="count"><i class="icon-twitter-sign"></i> Tweet</span></a></li>
            <li class="googleplus"><a href="https://plus.google.com/share?url=/papers/2015/04/11/speed-of-mini-batch-sgd.html" title="Share on Google Plus"><span class="count"><i class="icon-google-plus-sign"></i> +1</span></a></li>
          </ul>
        </div><!-- /.social-share -->
      </footer>
    </div><!-- /.entry-content -->
    
    
    <div class="read-more">
      
        <div class="read-more-header">
          <a href="" class="read-more-btn">Read More</a>
        </div><!-- /.read-more-header -->
        <div class="read-more-content">
          <h3><a href="/resource/2015/06/05/datasets.html" title="Datasets">Datasets</a></h3>
          <p>Here are some selected datasets commonly used in Machine Learning related tasks, for further automatical machine learning framework.**Git...&hellip; <a href="/resource/2015/06/05/datasets.html">Continue reading</a></p>
        </div><!-- /.read-more-content -->
      
      <div class="read-more-list">
        
          <div class="list-item">
            <h4><a href="/papers/2015/05/23/Adapations-and-Variations-of-Word2Vec.html" title="Adapations and Variations of Word2vec">Adapations and Variations of Word2vec</a></h4>
            <span>Published on May 23, 2015</span>
          </div><!-- /.list-item -->
        
          <div class="list-item">
            <h4><a href="/papers/2015/05/03/Improving-Word-Representations-via-Global-Context-and-Multiple-Word-Prototypes.html" title="Improving Word Representations via Global Context and Multiple Word Prototypes">Improving Word Representations via Global Context and Multiple Word Prototypes</a></h4>
            <span>Published on May 03, 2015</span>
          </div><!-- /.list-item -->
        
      </div><!-- /.read-more-list -->
      
    </div><!-- /.read-more -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2015 Maggie. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="/assets/js/scripts.min.js"></script>

<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = ''; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>	        

</body>
</html>
