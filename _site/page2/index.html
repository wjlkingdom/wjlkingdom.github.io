<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Blog &#8211; Yanran's Attic</title>
<meta name="description" content="Natural Language Processing, Machine Learning, Deep Learning, R">
<meta name="keywords" content="Natural Language Processing, Machine Learning, Deep Learning, Text Mining, R, Theano, GPU">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Blog">
<meta property="og:description" content="Natural Language Processing, Machine Learning, Deep Learning, R">
<meta property="og:url" content="http://yanran.li/page2/index.html">
<meta property="og:site_name" content="Yanran's Attic">





<link rel="canonical" href="http://yanran.li/page2/">
<link href="http://yanran.li/feed.xml" type="application/atom+xml" rel="alternate" title="Yanran's Attic Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://yanran.li/assets/css/main.min.css">
<!-- Webfonts -->
<link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://yanran.li/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://yanran.li/images/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://yanran.li/images/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://yanran.li/images/favicon.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://yanran.li/images/favicon.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://yanran.li/images/favicon.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://yanran.li/images/favicon.png">
<link rel="shortcut icon" href="/images/favicon.ico"/>
<link rel="bookmark" href="/images/favicon.ico"/>



<!-- MathJax -->
<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://yanran.li">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://yanran.li/images/avatar.jpg" alt="Yanran Li photo" class="author-photo">
					<h4>Yanran Li</h4>
					<p>Research Assistant at PolyU</p>
				</li>
				<li><a href="http://yanran.li/about/">Learn More</a></li>
				<li>
					<a href="mailto:yanranli.summer@gmail.com"><i class="icon-envelope"></i> Email</a>
				</li>
				
				
				<li>
					<a href="http://weibo.com/summerrlee"><i class="icon-weibo"></i> Weibo</a>
				</li>
				
				<li>
					<a href="https://www.linkedin.com/profile/view?id=233043238"><i class="icon-linkedin"></i> LinkedIn</a>
				</li>
				<li>
					<a href="http://github.com/niangaotuantuan"><i class="icon-github-alt"></i> GitHub</a>
				</li>
				
				
				
				
<!-- 				 -->
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://yanran.li/posts/">All Posts</a></li>
				<li><a href="http://yanran.li/categories/">All Categories</a></li>			
				<li><a href="http://yanran.li/tags/">All Tags</a></li>			
			</ul>
		</li>
		<li><a href="http://yanran.li/categories/#peppypapers">PaperNotes</a></li><li><a href="http://yanran.li/guestbook">GuestBook</a></li><li><a href="http://yanran.li/friends">Friends</a></li><li><a href="https://web.cs.dal.ca/~vlado/nlp/" class="external">NLP links</a></li><li><a href="http://www.reddit.com/r/machinelearning" class="external">r/machinelearning</a></li><li><a href="https://plus.google.com/communities/112866381580457264725" class="external">Deep Learning G+</a></li>
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  
  
    <div class="entry-image">
      <img src="http://yanran.li/images/bg_main.jpg" alt="Blog">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>Yanran's Attic</h1>
      <h2>Blog</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-09-27T00:00:00+08:00"><a href="http://yanran.li/naturallanguageprocessing/2013/09/27/from-word-embedding-to-language-model_1.html">September 27, 2013</a></time></span><span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://yanran.li/naturallanguageprocessing/2013/09/27/from-word-embedding-to-language-model_1.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://yanran.li/naturallanguageprocessing/2013/09/27/from-word-embedding-to-language-model_1.html" rel="bookmark" title="From Word Embedding to Language Model(1)" itemprop="url">From Word Embedding to Language Model(1)</a></h1>
    
  </header>
  <div class="entry-content">
    <h2 id="deep-learning-basement">Deep Learning Basement</h2>

<p>There are some basic important concepts in deep learning. </p>

<h3 id="backpropagation-network">Backpropagation Network</h3>

<p>BP Network is a special case of <strong>Feedforward networks</strong>, constituted by <strong>nonlinear continuous transformation</strong> units, and adjusted by <strong>error back propagation algorithm</strong>. The BP algorithm can be divided into two phases: propagation and weight update.</p>

<p>Also, some techniques are used to improve BP algorithm. For instance, gentic algorithm (supervised) and RBM (unsupervised), Auto-decoder (Deep Learning) can be in order to search for good weights before training.</p>

<h3 id="distributed-representations">Distributed Representations</h3>

<p>In contrast to the “atomic” or “localist” representations employed in traditional cognitive science, a distributed representation is one in which <strong>“each entity is represented by a pattern of activity distributed over many computing elements, and each computing element is involved in representing many different entities.”</strong></p>

<p>Yoshua Bengio gave a vivid analogy between “local” and “distributed” in his given talk, <em>Learning to Represent Semantics</em>.</p>

<p><img src="http://i.imgur.com/b8sEQFd.png" alt="" /></p>

<h2 id="deep-learning-motivation-for-semantics">Deep Learning Motivation for Semantics</h2>

<p>In Language Model, given a probability for a longer word sequence, 
<script type="math/tex"> P(w_{1},...,w_{l})=\prod_{t}P(w_{t}|w{t-1},...,w_{t-n+1}) </script>, 
we then predict P(next word|context). And in traditional n-gram Language Model, we use counts and smoothing to calculate the conditional probability. </p>

<p>However, the traditional n-gram LM fails with <em>Curse of dimensionality</em>: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training.</p>

<p>For example, the training sentence:</p>

<pre><code>The cat is walking in the bedroom.
</code></pre>

<p>The Test sentence:</p>

<pre><code>A dog was running in a room.
</code></pre>

<p>Under the sparsity/curse of dim. problem, we seek for a <strong>similar representations for semantically similar phrases</strong>.</p>

<h3 id="word-embedding-for-representing-words">Word Embedding for Representing Words</h3>

<p><strong>Word Embedding</strong> gives a low dimension (usually 50) distributed representation (Hinton, 1986) for each word. Thus similar words have similar representations. This distributed continuous-valued vector for each word can be learned from raw text (Colobert &amp; Weston, 2008).</p>

<h3 id="composing-words-with-nn">Composing Words with NN</h3>

<p>The Word Embedding is only the representation for each word, we should learn how to compose words into phrases and semantic relations. And Theorems on advantage of depth  (Hastad et al 86 &amp; 91, Bengio et al 2007, Bengio &amp;
Delalleau 2011, Braverman 2011) proves that deep architectures are more expressive and sharing components exponentially strengthens the advantage.</p>

<h2 id="neural-network-language-model">Neural Network Language Model</h2>

<p>Training the word embedding and the neural network at the same time, there are two types of system: Neural network language model and the others. The former type consists of (1)input: context; (2)output: distribution of next word, <script type="math/tex">\vert V\vert</script> nodes. And the latter type (1)input: entire sequence; (2)output: score, 1 node.</p>

<h3 id="probabilistic-neural-language-model">Probabilistic Neural Language Model</h3>

<table>
  <tbody>
    <tr>
      <td>Back to the problem, predict P(next word</td>
      <td>context). To calcute the conditional probability:</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Traiditional N-gram Languge Model uses counts and smoothing.</li>
  <li>Probabilistic Neural Language Model uses word embedding and neural network.</li>
</ul>

<p>After building the language model, compute word vectors during this process:</p>

<script type="math/tex; mode=display"> f(i,w_{t-1},...,w_{t-n+1})=g(i,C(w_{t-1}),...,C(w_{t-n+1})) </script>

<p><img src="http://i.imgur.com/gLGi5vU.png" alt="" /></p>

<p>This structure is Bengio’s groundbreaking work.
It has a linear projection layer, a nonlinear hidden layer and a softmax output layer. The sparse history h is projected into some continuous low-dimensional space, where similar histories get clustered. Moreover, the model is more robust: less parameters have to be estimated from the training data.</p>

<p>But the model has limitation below:</p>

<ul>
  <li>Complexity: <script type="math/tex"> (n × m) × h + h × \vert V\vert </script></li>
  <li>New words fails</li>
  <li>Long term context ignored</li>
  <li>Lack priori knowledge, such as POS, semantic information (WordNet)</li>
</ul>

<h2 id="problems-remain">Problems Remain</h2>

<p>How to design a neural network? An art or a science? :P</p>

<h2 id="references">References</h2>

<p>Yoshua Bengio. Learning to Represent Semantics. Words2Actions Workshop, NAACL HLT 2012, Montreal</p>

<p>Hinton, G. E., McClelland, J. L., and Rumelhart, D. E. (1986). Distributed representations. In Rumelhart, D. E. and McClelland, J. L., editors, Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations, MIT Press, Cambridge, MA.</p>

<p>R. Collobert and J. Weston. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In International Conference on Machine Learning, ICML, 2008.</p>

<p>Morin and Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS 2005.</p>

<p>Mnih and Hinton. A Scalable Hierarchical Distributed Language Model. NIPS 2008.</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-07-14T00:00:00+08:00"><a href="http://yanran.li/machinelearning/2013/07/14/basics-for-regularization.html">July 14, 2013</a></time></span><span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://yanran.li/machinelearning/2013/07/14/basics-for-regularization.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://yanran.li/machinelearning/2013/07/14/basics-for-regularization.html" rel="bookmark" title="Basics for Regularization" itemprop="url">Basics for Regularization</a></h1>
    
  </header>
  <div class="entry-content">
    <h2 id="overfittingregularization">为什么会有overfitting，为什么regularization有效</h2>
<p>对于某一个给定的 <script type="math/tex">\mathcal{F}</script> ，根据大数定理，当 n 趋向于无穷时，经验风险泛函是（依概率）收敛于风险泛函的。
但是机器学习是在一个给定的函数空间 <script type="math/tex">\mathcal{H}</script> 中搜索一个最优的（使得风险泛函最小的）函数，因此为了保证这个搜索过程是合理的，需要保证对于整个空间 <script type="math/tex">\mathcal{H}</script> 中的所有函数能一致收敛。
于是，当n趋于无穷时表现好并不是就代表 n 有限的时候同样好，即在 n 有限时，直接去最小化经验风险泛函并不一定是最优的，会造成 overfitting 等问题。所以，要分析在n 有限时，给出一个 <script type="math/tex">\mathcal{R_{P_n}}</script> 和 <script type="math/tex">\mathcal{R_P}</script> 之间的差别的 bound ，这个 bound 不仅依赖于 n 的大小，还依赖于我们所用的目标函数空间的“大小”——比如用 VC 维之类的东西来刻画。因此，在最小化经验风险泛函的同时，通过正则化的方法同时去最小化目标函数空间的“大小”——即“结构风险最小化”。</p>

<h2 id="regularization">Regularization带来什么</h2>
<p>这部分是看《Learning From Data》的slides里讲到的。</p>

<p>Regularization 其实是 constrain 了搜索空间，比如 constrain 了最小二乘的 weight 大小。
带来的结果是 bias 有可能增加（side-effect），但 variance 降低（这是期望带来的）。</p>

<h2 id="l0-l1-l2">L0, L1, L2都带来什么</h2>
<p>L0应该就是SRM，结构风险最小化。据说可以用来筛掉指数级别的不相关feature，但是不可求解。</p>

<p>(1) <script type="math/tex"> \min_{w} 1/n \sum_{i=1}^{n}l(y,f_{w}(x))+\lambda count\left \{ {w_{j}\neq 0} \right \} </script></p>

<p>L1也是NP-hard问题（本质因为可以和L0对等），是个菱形，最初用于所谓的compressed sensing。</p>

<p>(2) <script type="math/tex"> \min_{w} 1/n \sum_{i=1}^{n}l(y,f_{w}(x))+\lambda \left \| w \right \|_{1} </script></p>

<p>L2是个球形，保持旋转不变性。</p>

<p>(3) <script type="math/tex"> \min_{w} 1/n \sum_{i=1}^{n}l(y,f_{w}(x))+\lambda \left \| w \right \|_{2} </script></p>

<p>L1和L2的话，L1可以作为特征选择（本质也是因为对等到L0，而L0中是非零分量个数的正则），但是比较难求解（难是说需要用优化算法求解，比如Bregman Iteration）；L2求解方便，不会引入很高的复杂度。</p>

<p>关于L1和L0优化的等价问题可以参考 Candes，Donoho 等人的 Compressed sensing 理论。</p>

<p>当然还有什么L1/2之类的，是 non-convex 的。先不说了。</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-07-03T00:00:00+08:00"><a href="http://yanran.li/statistics/2013/07/03/covariate-shift-correction.html">July 03, 2013</a></time></span><span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://yanran.li/statistics/2013/07/03/covariate-shift-correction.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://yanran.li/statistics/2013/07/03/covariate-shift-correction.html" rel="bookmark" title="Covariate Shift Correction" itemprop="url">Covariate Shift Correction</a></h1>
    
  </header>
  <div class="entry-content">
    <p><a href="http://weibo.com/bit9">江申</a>在《DSP中的算法初探》中提到了一个 Covariate Shift 的问题，第一次听说，查了一下，发现是一个非常重要的问题。</p>

<p>在 DSP 中，这个问题叫做 Bid Adjustment: </p>

<pre><code>在线上生产环境进行实际竞价时，通常需要对竞价模型的参数做调整。
</code></pre>

<p>原因：1）线上的数据分布与线下用的训练数据的分布不一样，需要对参数做调整；
2）线上的环境是动态变化的，得让参数也随之变化。</p>

<p><strong>Covariate Shift</strong> 就是说， training and test data were so different，<strong>我们在 training 过程中 sampling 假设的 distribution 和实际真实的 distribution 差异太大了</strong>导致我们最后的training 是 waste。</p>

<p>解决办法是通过两步：</p>

<ul>
  <li>Step 1: 得到真实分布 q 和假设的分布 p 之间的 ratio</li>
  <li>Step 2: reweight training set</li>
</ul>

<p>Step 1就需要考虑如何去衡量两个分布之间的差异。直观的方法是：训练一个 LR 模型，数据为“训练+待预测”数据，Label 为是否属于训练集。分得准，差异大。分不准，差异小。
理论上这里 用 任意learning方法出来的 classifier 都是可以的（见 paper: <a href="http://jmlr.org/papers/volume10/bickel09a/bickel09a.pdf最后的conclusion">Discriminative Learning Under Covariate Shift</a> conclusion 的部分）。</p>

<p>特别的，如果用LR的话，简单的推导见 Alex Smola 的一篇 <a href="http://blog.smola.org/post/4110255196/real-simple-covariate-shift-correction">blog</a>。</p>

<ul>
  <li>Step 2 学到了这个 ratio 就可以做 reweight。</li>
</ul>

<p>re-weight each instance by the ratio of probabilities that it would have been drawn from the correct distribution, that is, we need to reweight things by p(xi)q(xi). This is the ratio of how frequently the instances would have occurred in the correct set vs. how frequently it occurred with the sampling distribution q. </p>

<p>很多 Transfer Learning 的方法都和这个类似。</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-06-27T00:00:00+08:00"><a href="http://yanran.li/r/2013/06/27/loading-data-in-R.html">June 27, 2013</a></time></span><span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://yanran.li/r/2013/06/27/loading-data-in-R.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://yanran.li/r/2013/06/27/loading-data-in-R.html" rel="bookmark" title="Loading Data in R" itemprop="url">Loading Data in R</a></h1>
    
  </header>
  <div class="entry-content">
    <p>There are several types of problems one may meet when loading data in R. I solved some of them and taken down the notes below.</p>

<h4 id="tough-garbage-cn-characters-in-r">tough garbage CN characters in R</h4>
<p>中文乱码的问题在很多情况下都遇到了。内因是R是用本地码（通常是GBK）来解释unicode。 
目测整体解决办法有几种： </p>

<ul>
  <li>Encodings </li>
</ul>

<p>这个办法我只成功解决过一次。 </p>

<pre><code>source(file,encoding="utf-8") 
</code></pre>

<ul>
  <li>
    <p>改R的环境 </p>

    <p>很奇怪的是在英文环境下都反而有时候不乱码。 </p>
  </li>
  <li>
    <p>操作系统的系统编码问题 </p>

    <p>Windows是 gbk 编码，且不可改！（所以只能 Encodings 改了）；Linux 是 utf-8 。可以用 sessioninfo() 来查看 locale 的编码，然后改掉。一般有时候比如 mysql 也乱码的时候这个方法很好用，应该是个通用性很高的方法。 </p>

    <p><em>Windows</em> 
  一般是gbk的编码，读取utf-8的文件时，需要声明读取编码就OK了。 </p>

    <pre><code>  source(file,encoding="utf-8") 
</code></pre>

    <p><em>Linux</em>的情况复杂一些 
  * locale要设置成zh_CN 
  * 要安装中文字符集，或者从window下复制过去 
  * R读取，统一用utf-8的。 </p>

    <p>最复杂的情况是<em>DB连接</em> 
  * 有时候DB的字符集是gb2312, gbk, utf8等 
  * 在DB读取的时候，DBI包，要设置DB的字符编码 
  * 当把数据读到R中时，要跟R的环境的编码要统一 
  * linux/win两套环境，编码部分要是区别写的。 </p>

    <p><a href="http://f.dataguru.cn/thread-20496-1-1.html">Ref</a> </p>
  </li>
  <li>
    <p>强大的iconv() </p>

    <p><a href="http://stat.ethz.ch/R-manual/R-patched/library/base/html/iconv.html"><em>Usage</em></a></p>

    <pre><code>  iconv(x, from = "", to = "", sub = NA, mark = TRUE) 
  iconvlist()  
</code></pre>

    <p>除此以外还可以用于除掉一些乱码，比如 Removing non-ASCII characters.  <a href="http://stackoverflow.com/questions/9934856/removing-non-ascii-characters-from-data-files">Ref</a> </p>
  </li>
  <li>
    <p>强大的iconv()也失效时</p>

    <ul>
      <li>
        <p>更多更好的去理解网页编码 <a href="http://yishuo.org/r/2012/09/13/junk-code-again.html">Ref</a></p>

        <pre><code>  url= htmlParse(url,encoding="UTF-8")  
</code></pre>
      </li>
      <li>
        <p>embedded null characters (‘\0’) in strings </p>
      </li>
    </ul>

    <p>这个似乎也是个 devils 在 inferno 的书里有写，下次再开坑吧。 <a href="http://biostatmatt.com/archives/456">Ref</a> </p>
  </li>
</ul>

<h4 id="missing-values">missing values</h4>

<p>大概是 missing value 要仔细处理。 </p>

<p>和 missing value 有关的大概有4件事： </p>

<ul>
  <li>如何填充 missing value </li>
  <li>misquote 等等会引起 missing value </li>
  <li>whitespace 可能丧失 </li>
  <li>
    <p>extraneous fields 用 fill 解决或者用 count.fields 诊断</p>

    <pre><code>  x &lt;- count.fields("UserProfile.tsv", sep = '\t') 
  table(x) 
  which(x != legal.length) // check where the illegal lines are 
	
	
  userlist &lt;- read.table("UserProfile.tsv", sep = '\t', header = FALSE, stringAsFactors = FALSE, fill = TRUE) // "file" matters. 
</code></pre>
  </li>
</ul>

<p>其中填充 missing value 涉及到 na.strings()。这里牵扯到如果一个 string value 真的是 NA，要注意加quote。 <a href="https://science.nature.nps.gov/im/datamgmt/statistics/r/fundamentals/manipulation.cfm">Ref</a> </p>

<p>再之， 对 NA 的问题又牵扯出 <a href="http://www.ats.ucla.edu/stat/r/faq/missing.htm">na.action</a>.</p>

<h4 id="group-to-summary">group to summary</h4>

<ul>
  <li>The ddply() function. It is the easiest to use, though it requires the plyr package. This is probably what you want to use. </li>
  <li>The summarizeBy() function. It is easier to use, though it requires the doBy package. </li>
  <li>The aggregate() function. It is more difficult to use but is included in the base install of R. </li>
</ul>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2011-07-01T00:00:00+08:00"><a href="http://yanran.li/bookreviews/2011/07/01/how-to-train-critical-thinking.html">July 01, 2011</a></time></span><span class="author vcard"><span class="fn"><a href="http://yanran.li/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="http://yanran.li/bookreviews/2011/07/01/how-to-train-critical-thinking.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://yanran.li/bookreviews/2011/07/01/how-to-train-critical-thinking.html" rel="bookmark" title="如何训练批判性思维" itemprop="url">如何训练批判性思维</a></h1>
    
  </header>
  <div class="entry-content">
    <p>《学会提问——批判性思维指南》 
　　 
<img src="http://img3.douban.com/lpic/s1509100.jpg" alt="" /></p>

<h2 id="section">步骤</h2>

<p>1、 “谁关心这个话题” ：你的时间是宝贵的，在你花费时间对某个问题进行批判性评价前，先问问who cares。</p>

<p>2、找出结论 </p>

<p>　　	在找出结论之前我们无法做出批判性评价！ </p>

<p>　　	结论就是演讲者或作者希望你接受的信息。 </p>

<p>　　	没有支持的言论仅仅是一些观点而非结论。 </p>

<p>3、确定理由是批判性思维的重要步骤</p>

<p>　　
	要问“为什么”：要确定你是否发现了一条理由，最好的方式是你尽量扮演作者的角色。</p>

<p>　　	结论依赖于理由的价值。 </p>

<p>4、寻找干扰性原因 </p>

<p>　　	当你有充足的理由相信作者或演说者对某件事的因果解释的证据时，你就需要寻找干扰性原因。 </p>

<p>　　	一旦你发现一个因果说明，一定要警惕干扰性原因存在的可能性。 </p>

<p>5、被遗漏的信息 
　　
	我们要特别强调下面这种被遗漏的信息，因为这种信息非常重要，但却常常被忽略，这就是：作者所提倡的行为可能产生的消极作用。 
　　 </p>

<h2 id="section-1">概念类</h2>

<h3 id="section-2">两种思维——海绵式思维、淘金式思维</h3>

<p>1、海绵式思维：吸收、获得 </p>

<p>　　优点1：吸收的越多，越能理解它的复杂性。 </p>

<p>　　优点2：相对被动，主要的心理加工就是注意和记忆。 </p>

<p>　　缺点1：始终相信其最后接收的信息 </p>

<p>2、淘金式思维：与知识积极的互动 </p>

<p>　　优点1：评判所见所闻的价值 </p>

<p>　　优点2：回报巨大 </p>

<p>　　缺点1：艰辛、具有挑战性 
　　</p>

<h3 id="section-3">强、弱批判性思维</h3>

<p>1、弱——是用批判性思维维护你自己已有的观点 </p>

<p>2、强——是运用相同的技能来评估所有的观点和信念，特别是评估自己的观点和信念</p>

<h3 id="section-4">论题的种类</h3>

<p>1、描述性论题——针对有关过去、现在、未来的描述是否正确提出的问题。 </p>

<p>2、说明性论题——针对我们应当怎样做及对与错、好与提出的问题。 </p>

<h3 id="section-5">文章基本要素：论题、结论、理由</h3>

<p>理由包括信念、证据、比喻、类推以及其他用来支持或证明这些结论的陈述。 </p>

<h3 id="section-6">两种假设</h3>
<p>　　
1、描述性假设——关于世界是什么样子的观念 </p>

<p>2、说明性或价值观假设——关于世界应当怎样的观念</p>

<h3 id="section-7">三个通常的谬误</h3>

<p>1、提供了错误或不正确假设的推理 </p>

<p>2、通过使信息看起来与结论相关而实际上不相关来转移我们的视线</p>

<p>3、需要使用已经被证实为真的结论来为结论提供支持 </p>

<h3 id="section-8">书摘</h3>

<blockquote>

  <p>　　1、	人类的行为是如此的矛盾和复杂，因此我们对人类行为有问题的最好答案在本质上就带有不确定性。    <br />
　　2、	旧答案和新答案之间的相互影响是我们成长的基础。     <br />
　　3、	决定是否赞同某个观点的根本一步就是确定关键词或关键句的准确含义。 
　　4、	政治性的语言常常附带有感情色彩，并有歧义。      <br />
　　5、	请记住：你的听众不能长时间地专注于你说的话。如果你让某个听众感到困惑，那么你可能很快就会失去他；如果你不能重新吸引他的注意，你就是一个失败的传达者。 
　　6、	在所有论证中，都存在一些作者所认同的思想，而这类思想的典型特征就是作者没有对它们进行清晰的陈述。 
　　7、	在推理的结构中，这些思想是隐形的重要环节，是将全部论证整合在一起的黏合剂。 
　　8、	事实上，只有当推理中加入价值观假设时，作者的理由才能在逻辑上支持结论。 
　　9、	同一个价值观对不同的人来说，强烈程度是不同的。在回答一个说明性问题时，价值观的这种相对强度就会导致你得出与别人不同的答案。 
　　10、	我们仅仅在某一个点上保持自己的价值观倾向。 
　　11、	即使人们具有相同的价值观假设，也有可能得出不同的结论，因为人们对于每种结果产生的可能性及其影响程度有着不一致的看法。 
　　12、	你要记住，由于诸多原因，权威的意见常常是错误的。 
　　13、	记住：两事件相关并不能证明它们之间有因果关系！ 
　　14、	另外，“基本归因错误”是一种常见的偏见，即在解释他人的行为时，我们会过高地估计个人倾向性的作用，而降低了环境因素的作用。 
　　15、	推理往往是不完整的。因此，如果你一遇到信息缺失就自动声明你无法得出结论，那么，你就永远无法形成任何观点。 </p>
</blockquote>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->


<div class="pagination">
  
    
      <a href="http://yanran.li" class="btn">Previous</a>
    
  
  <ul class="inline-list">
    <li>
      
        <a href="http://yanran.li">1</a>
      
    </li>
    
      <li>
        
          <span class="current-page">2</span>
        
      </li>
    
  </ul>
  
    Next
  
</div><!-- /.pagination -->
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2015 Yanran Li. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://yanran.li/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://yanran.li/assets/js/scripts.min.js"></script>

          

</body>
</html>